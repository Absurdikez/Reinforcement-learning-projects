{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"1Aq5zGFQZ-kzPvlcmNfV9xV-AZE1S9YeR","timestamp":1667315045113},{"file_id":"1-z1MhF8InRJTlhNdmrlHgJdYj5C7pXMg","timestamp":1666606240331}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"IAE8kMxe6E6k"},"source":["# MVA - Homework 1 - Reinforcement Learning (2022/2023)\n","\n","**Name:** TOPCU Zeki"]},{"cell_type":"markdown","metadata":{"id":"vY4MH0nU637o"},"source":["## Instructions\n","\n","* The deadline is **November 10 at 11:59 pm (Paris time).**\n","\n","* By doing this homework you agree to the late day policy, collaboration and misconduct rules reported on [Piazza](https://piazza.com/class/l4y5ubadwj64mb/post/6).\n","\n","* **Mysterious or unsupported answers will not receive full credit**. A correct answer, unsupported by calculations, explanation, or algebraic work will receive no credit; an incorrect answer supported by substantially correct calculations and explanations might still receive partial credit.\n","\n","* Answers should be provided in **English**."]},{"cell_type":"markdown","metadata":{"id":"YB__2uUC5U1r"},"source":["# Colab setup"]},{"cell_type":"code","metadata":{"id":"2XNj1_VZ2FGJ"},"source":["from IPython import get_ipython\n","\n","if 'google.colab' in str(get_ipython()):\n","  # install rlberry library\n","  !pip install git+https://github.com/rlberry-py/rlberry.git@mva2021#egg=rlberry[default] > /dev/null 2>&1\n","\n","  # install ffmpeg-python for saving videos\n","  !pip install ffmpeg-python > /dev/null 2>&1\n","\n","  # packages required to show video\n","  !pip install pyvirtualdisplay > /dev/null 2>&1\n","  !apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n","\n","  print(\"Libraries installed, please restart the runtime!\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s8F7RiPXjutB"},"source":["# Create directory for saving videos\n","!mkdir videos > /dev/null 2>&1\n","\n","# Initialize display and import function to show videos\n","import rlberry.colab_utils.display_setup\n","from rlberry.colab_utils.display_setup import show_video"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KISV44N_nCNm"},"source":["# Useful libraries\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import numpy"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L4hKBRTCh6Gk"},"source":["# Preparation\n","\n","In the coding exercises, you will use a *grid-world* MDP, which is represented in Python using the interface provided by the [Gym](https://gym.openai.com/) library. The cells below show how to interact with this MDP and how to visualize it.\n"]},{"cell_type":"code","metadata":{"id":"514mHDeQooKa"},"source":["from rlberry.envs import GridWorld\n","\n","def get_env():\n","  \"\"\"Creates an instance of a grid-world MDP.\"\"\"\n","  env = GridWorld(\n","      nrows=5,\n","      ncols=7,\n","      reward_at = {(0, 6):1.0},\n","      walls=((0, 4), (1, 4), (2, 4), (3, 4)),\n","      success_probability=0.9,\n","      terminal_states=((0, 6),)\n","  )\n","  return env\n","\n","def render_policy(env, policy=None, horizon=50):\n","  \"\"\"Visualize a policy in an environment\n","\n","  Args:\n","    env: GridWorld\n","        environment where to run the policy\n","    policy: np.array\n","        matrix mapping states to action (Ns).\n","        If None, runs random policy.\n","    horizon: int\n","        maximum number of timesteps in the environment.\n","  \"\"\"\n","  env.enable_rendering()\n","  state = env.reset()                       # get initial state\n","  for timestep in range(horizon):\n","      if policy is None:\n","        action = env.action_space.sample()  # take random actions\n","      else:\n","        action = policy[state]\n","      next_state, reward, is_terminal, info = env.step(action)\n","      state = next_state\n","      if is_terminal:\n","        break\n","  # save video and clear buffer\n","  env.save_video('./videos/gw.mp4', framerate=5)\n","  env.clear_render_buffer()\n","  env.disable_rendering()\n","  # show video\n","  show_video('./videos/gw.mp4')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hQAHUBw_ifMI"},"source":["# Create an environment and visualize it\n","env = get_env()\n","render_policy(env)  # visualize random policy\n","\n","# The reward function and transition probabilities can be accessed through\n","# the R and P attributes:\n","print(f\"Shape of the reward array = (S, A) = {env.R.shape}\")\n","print(f\"Shape of the transition array = (S, A, S) = {env.P.shape}\")\n","print(f\"Reward at (s, a) = (1, 0): {env.R[1, 0]}\")\n","print(f\"Prob[s\\'=2 | s=1, a=0]: {env.P[1, 0, 2]}\")\n","print(f\"Number of states and actions: {env.Ns}, {env.Na}\")\n","\n","# The states in the griworld correspond to (row, col) coordinates.\n","# The environment provides a mapping between (row, col) and the index of\n","# each state:\n","print(f\"Index of state (1, 0): {env.coord2index[(1, 0)]}\")\n","print(f\"Coordinates of state 5: {env.index2coord[5]}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ibGD_3I89CNu"},"source":["# Part 1 - Dynamic Programming"]},{"cell_type":"markdown","metadata":{"id":"NR7h5won9NQY"},"source":["## Question 1.1\n","\n","Consider a general MDP with a discount factor of $\\gamma < 1$. Assume that the horizon is infinite (so there is no termination). A policy $\\pi$ in this MDP\n","induces a value function $V^\\pi$. Suppose an affine transformation is applied to the reward, what is\n","the new value function? Is the optimal policy preserved?\n","\n"]},{"cell_type":"markdown","metadata":{"id":"313W4K3B_LtN"},"source":["### **Answer**\n","\n","[your answer here]"]},{"cell_type":"markdown","metadata":{"id":"0uCVgkDo9vTM"},"source":["## Question 1.2\n","\n","Consider an infinite-horizon $\\gamma$-discounted MDP. We denote by $Q^*$ the $Q$-function of the optimal policy $\\pi^*$. Prove that, for any function $Q(s, a)$ (which is **not** necessarily the value function of a policy), the following inequality holds for any state $s$:\n","\n","$$\n","V^{\\pi_Q}(s) \\geq V^*(s) - \\frac{2}{1-\\gamma}||Q^*-Q||_\\infty,\n","$$\n","\n","where $||Q^*-Q||_\\infty = \\max_{s, a} |Q^*(s, a) - Q(s, a)|$ and $\\pi_Q(s) \\in \\arg\\max_a Q(s, a)$. Can you use this result to show that any policy $\\pi$ such that $\\pi(s) \\in \\arg\\max_a Q^*(s, a)$ is optimal?"]},{"cell_type":"markdown","metadata":{"id":"8MqGWPPD_OAI"},"source":["### **Answer**\n","\n","[your answer here]"]},{"cell_type":"markdown","metadata":{"id":"yIrtb7sihYcM"},"source":["## Question 1.3\n","\n","In this question, you will implement and compare the policy and value iteration algorithms for a finite MDP. \n","\n","Complete the functions `policy_evaluation`, `policy_iteration` and `value_iteration` below.\n","\n","\n","Compare value iteration and policy iteration. Highlight pros and cons of each method."]},{"cell_type":"markdown","metadata":{"id":"GLmQtk-wt0HS"},"source":["### **Answer**\n","\n","[pros/cons of each method + implementation below]"]},{"cell_type":"code","metadata":{"id":"9yI0YYtMmpDQ"},"source":["from numpy.matrixlib.defmatrix import N\n","\n","def policy_evaluation(P, R, policy, gamma=0.9, tol=1e-2):\n","    \"\"\"\n","    Args:\n","        P: np.array\n","            transition matrix (NsxNaxNs)\n","        R: np.array\n","            reward matrix (NsxNa)\n","        policy: np.array\n","            matrix mapping states to action (Ns)\n","        gamma: float\n","            discount factor\n","        tol: float\n","            precision of the solution\n","    Return:\n","        value_function: np.array\n","            The value function of the given policy\n","    \"\"\"\n","    Ns, Na = R.shape #state length, action length\n","    V = np.zeros(Ns) #we initialize value functions for each state\n","    while True:\n","      delta = 0\n","      #for each state, performing a \"full backup\"\n","      for s in range(Ns):\n","        v=0\n","        #Look for possible next actions from policy\n","        for a,a_prob in enumerate(policy[s]):\n","          #for each action, look at possible nex states\n","          for next_state in range(Ns):\n","            v+=a_prob*P[s,a,next_state]*(R[s,a]+gamma*V[next_state])\n","        #how much value function changed\n","        delta = max(delta, np.abs(v-V[s]))\n","        V[s]=v\n","      if delta<tol:\n","        break\n","    return np.array(V)\n","\n","Ns,Na=env.R.shape\n","policy_evaluation(env.P,env.R,np.ones([Ns,Na])/Na)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from ast import Name\n","from numpy.core.fromnumeric import argmax\n","from scipy.optimize import minimize\n","import numpy as np\n","def policy_iteration(P,R, gamma=0.9,tol=1e-3):\n","\n","    Ns,Na=R.shape\n","\n","#supportive function to look one step ahead\n","    def one_step_lookahead(state, V):\n","\n","        A = np.zeros(Na)\n","        for a in range(Na):\n","            for next_state in range(Ns):\n","                A[a] += P[state,a,next_state] * (R[state,a] + gamma * V[next_state])\n","        return A\n","    \n","    # Start with a random policy\n","    policy = np.ones([Ns,Na])/Na\n","\n","    while True:\n","        # Evaluate the current policy\n","        V = policy_evaluation(P,R,policy,gamma)\n","        \n","        # Will be set to false if we make any changes to the policy\n","        policy_stable = True\n","        \n","        # For each state\n","        for s in range(Ns):\n","            # The best action we would take under the current policy\n","            chosen_a = np.argmax(policy[s])\n","            \n","            # Find the best action by one-step lookahead\n","            action_values = one_step_lookahead(s, V)\n","            best_a = np.argmax(action_values)\n","            \n","            # Greedily update the policy\n","            if chosen_a != best_a:\n","                policy_stable = False\n","            policy[s] = best_a\n","        \n","        # If the policy is stable return it\n","        if policy_stable:\n","            return policy, V\n","\n","\n","def value_iteration(P,R,gamma=0.9,tol=1e-3):\n","    \"\"\" \n","    Args:\n","        P: np.array\n","            transition matrix (NsxNaxNs)\n","        R: np.array\n","            reward matrix (NsxNa)\n","        gamma: float\n","            discount factor\n","        tol: float\n","            precision of the solution\n","    Return:\n","        Q: final Q-function (at iteration n)\n","        greedy_policy: greedy policy wrt Qn\n","        Qfs: all Q-functions generated by the algorithm (for visualization)\n","    \"\"\"\n","    \n","    Ns, Na = R.shape\n","    Q = np.zeros((Ns, Na))\n","    Qfs = [Q]\n","    \n","    converged=False\n","    while not converged:\n","      delta = 0\n","      for s in range(int(Ns)):\n","        for a in range(int(Na)):\n","          tmp = Q[s,a]\n","          Q[s,a] = 0\n","          for n_s in range(int(Ns)):\n","            Q[s,a] += P[s,a,n_s] * (R[s,a] + gamma * np.max(Q[n_s]))\n","          delta = np.maximum(delta, np.abs(tmp - Q[s,a]))\n","      Qfs.append(Q)\n","      converged = True if delta < tol else False\n","\n","    greedy_policy = np.argmax(Q,axis=1)\n","    #\n","\n","    return Q, greedy_policy, Qfs\n","\n"],"metadata":{"id":"55JMbMXMBpvT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-Fi0IzZJp74Z"},"source":["### Testing your code"]},{"cell_type":"code","metadata":{"id":"P7JKrc1oqFI2"},"source":["from matplotlib import pyplot as plt\n","\n","# Parameters\n","tol = 1e-5\n","gamma = 0.99\n","\n","# Environment\n","env = get_env()\n","\n","# run value iteration to obtain Q-values\n","VI_Q, VI_greedypol, all_qfunctions = value_iteration(env.P, env.R, gamma=gamma, tol=tol)\n","\n","print(VI_greedypol)\n","# render the policy\n","print(\"[VI]Greedy policy: \")\n","render_policy(env, VI_greedypol)  \n","\n","# compute the value function of the greedy policy using matrix inversion\n","# ====================================================\n","# YOUR IMPLEMENTATION HERE \n","# compute value function of the greedy policy\n","\n","#V_greedy = policy_evaluation(env.P,env.R,VI_greedypol)\n","\n","#\n","\n","# ====================================================\n","\n","# show the error between the computed V-functions and the final V-function\n","# (that should be the optimal one, if correctly implemented)\n","# as a function of time\n","final_V = all_qfunctions[-1].max(axis=1)\n","norms = [ np.linalg.norm(q.max(axis=1) - final_V) for q in all_qfunctions]\n","plt.plot(norms)\n","plt.xlabel('Iteration')\n","plt.ylabel('Error')\n","plt.title(\"Value iteration: convergence\")\n","\n","#### POLICY ITERATION ####\n","PI_policy, PI_V = policy_iteration(env.P, env.R, gamma=gamma, tol=tol)\n","print(\"\\n[PI]final policy: \")\n","render_policy(env, PI_policy)\n","\n","## Uncomment below to check that everything is correct\n","#assert np.allclose(PI_policy, VI_greedypol),\\\n","#\"You should check the code, the greedy policy computed by VI is not equal to the solution of PI\"\n","#np.allclose(PI_V, greedy_V),\\\n","#     \"Since the policies are equal, even the value function should be\"\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2V1QdoH-xFX0"},"source":["# Part 2 - Tabular RL"]},{"cell_type":"markdown","metadata":{"id":"Qf51VhoPxbV4"},"source":["## Question 2.1\n","\n","The code below collects two datasets of transitions (containing states, actions, rewards and next states) for a discrete MDP.\n","\n","For each of the datasets:\n","\n","1. Estimate the transitions and rewards, $\\hat{P}$ and $\\hat{R}$.\n","2. Compute the optimal value function and the optimal policy with respect to the estimated MDP (defined by $\\hat{P}$ and $\\hat{R}$), which we denote by $\\hat{\\pi}$ and $\\hat{V}$.\n","3. Numerically compare the performance of $\\hat{\\pi}$ and $\\pi^\\star$ (the true optimal policy), and the error between $\\hat{V}$ and $V^*$ (the true optimal value function).\n","\n","Which of the two data collection methods do you think is better? Why?"]},{"cell_type":"markdown","metadata":{"id":"eWSyewG2EZpJ"},"source":["### **Answer**\n","\n","[answer last question + implementation below]"]},{"cell_type":"code","metadata":{"id":"8lNPhB28EcGd"},"source":["def get_random_policy_dataset(env, n_samples):\n","  \"\"\"Get a dataset following a random policy to collect data.\"\"\"\n","  states = []\n","  actions = []\n","  rewards = []\n","  next_states = []\n","  \n","  state = env.reset()\n","  for _ in range(n_samples):\n","    action = env.action_space.sample()\n","    next_state, reward, is_terminal, info = env.step(action)\n","    states.append(state)\n","    actions.append(action)\n","    rewards.append(reward)\n","    next_states.append(next_state)\n","    # update state\n","    state = next_state\n","    if is_terminal:\n","      state = env.reset()\n","\n","  dataset = (states, actions, rewards, next_states)\n","  return dataset\n","\n","def get_uniform_dataset(env, n_samples):\n","  \"\"\"Get a dataset by uniformly sampling states and actions.\"\"\"\n","  states = []\n","  actions = []\n","  rewards = []\n","  next_states = []\n","  for _ in range(n_samples):\n","    state = env.observation_space.sample()\n","    action = env.action_space.sample()\n","    next_state, reward, is_terminal, info = env.sample(state, action)\n","    states.append(state)\n","    actions.append(action)\n","    rewards.append(reward)\n","    next_states.append(next_state)\n","\n","  dataset = (states, actions, rewards, next_states)\n","  return dataset\n","\n","\n","# Collect two different datasets\n","num_samples = 500\n","env = get_env()\n","dataset_1 = get_random_policy_dataset(env, num_samples)\n","dataset_2 = get_uniform_dataset(env, num_samples)\n","\n","\n","# Item 3: Estimate the MDP with the two datasets; compare the optimal value\n","# functions in the true and in the estimated MDPs\n","\n","# ...\n","\n","def estimate_p_r(env,dataset):\n","  Ns,Na = env.R.shape\n","  P = np.zeros([Ns,Na,Ns])\n","  R = np.zeros([Ns,Na])\n","\n","  for (state,action,reward,nextState) in np.array(dataset).T:\n","    P[int(state),int(action),int(nextState)]+=1 \n","    R[int(state),int(action)] += reward\n","  \n","  for state in range(Ns):\n","    for a in range(Na):\n","      N = max(1.,np.sum(P[int(state),int(action)])) #to divide the number of samples P and R\n","      R[int(state),int(action)] /= N\n","      P[int(state),int(action)] /= N\n","  return P,R\n","\n","P_data1, R_data1 = estimate_p_r(env,dataset_1)\n","\n","P_data2, R_data2 = estimate_p_r(env,dataset_2)\n","\n","optimalPolicy,optimalValueFunction=policy_iteration(env.P,env.R)\n","dataset_1Policy, dataset_1ValueFunction = policy_iteration(P_data1,R_data1)\n","\n","dataset_2Policy, dataset_2ValueFunction = policy_iteration(P_data2,R_data2)\n","\n","error_data1 = np.linalg.norm(optimalValueFunction - dataset_1ValueFunction, ord=np.inf)\n","error_data2 = np.linalg.norm(optimalValueFunction - dataset_2ValueFunction, ord=np.inf)\n","\n","print(\"Error of data set 1 is:\", error_data1)\n","print(\"Error of data set 2 is:\", error_data2)\n","\n","Ns = range(env.R.shape[0])\n","plt.title(\"Each policy visualization\")\n","plt.plot(Ns, optimalPolicy, label=\"optimal policy\")\n","plt.plot(Ns, dataset_1Policy, label=\"policy of dataset 1\")\n","plt.plot(Ns, dataset_2Policy, label=\"policy of dataset 2\")\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UKINsa_yGLGL"},"source":["## Question 2.2\n","\n","Suppose that $\\hat{P}$ and $\\hat{R}$ are estimated from a dataset of exactly $N$ i.i.d. samples from **each** state-action pair. This means that, for each $(s,a)$, we have $N$ samples $\\{(s_1',r_1, \\dots, s_N', r_N\\}$, where $s_i' \\sim P(\\cdot | s,a)$ and $r_i \\sim R(s,a)$ for $i=1,\\dots,N$, and\n","$$ \\hat{P}(s'|s,a) = \\frac{1}{N}\\sum_{i=1}^N \\mathbb{1}(s_i' = s'), $$\n","$$ \\hat{R}(s,a) = \\frac{1}{N}\\sum_{i=1}^N r_i.$$\n","Suppose that $R$ is a distribution with support in $[0,1]$. Let $\\hat{V}$ be the optimal value function computed in the empirical MDP (i.e., the one with transitions $\\hat{P}$ and rewards $\\hat{R}$). For any $\\delta\\in(0,1)$, derive an upper bound to the error\n","\n","$$ \\| \\hat{V} - V^* \\|_\\infty $$\n","\n","which holds with probability at least $1-\\delta$.\n","\n","**Note** Your bound should only depend on deterministic quantities like $N$, $\\gamma$, $\\delta$, $S$, $A$. It should *not* dependent on the actual random samples.\n","\n","**Hint** The following two inequalities may be helpful.\n","\n","1. **A (simplified) lemma**. For any state $\\bar{s}$,\n","\n","$$ |\\hat{V}(\\bar{s}) - V^*(\\bar{s})| \\leq \\frac{1}{1-\\gamma}\\max_{s,a} \\left| R(s,a) - \\hat{R}(s,a) + \\gamma \\sum_{s'}(P(s'|s,a) - \\hat{P}(s'|s,a)) V^*(s') \\right|$$\n","\n","2. **Hoeffding's inequality**. Let $X_1, \\dots X_N$ be $N$ i.i.d. random variables bounded in the interval $[0,b]$ for some $b>0$. Let $\\bar{X} = \\frac{1}{N}\\sum_{i=1}^N X_i$ be the empirical mean. Then, for any $\\epsilon > 0$,\n","\n","$$ \\mathbb{P}(|\\bar{X} - \\mathbb{E}[\\bar{X}]| > \\epsilon) \\leq 2e^{-\\frac{2N\\epsilon^2}{b^2}}.$$"]},{"cell_type":"markdown","metadata":{"id":"fKmdulLaMoiN"},"source":["### **Answer**\n","\n","[your derivation here]"]},{"cell_type":"markdown","metadata":{"id":"tpqwCBG2MwxO"},"source":["## Question 2.3\n","\n","Suppose once again that we are given a dataset of $N$ samples in the form of tuples $(s_i,a_i,s_i',r_i)$. We know that each tuple contains a valid transition from the true MDP, i.e., $s_i' \\sim P(\\cdot | s_i, a_i)$ and $r_i \\sim R(s_i,a_i)$, while the state-action pairs $(s_i,a_i)$ from which the transition started can be arbitrary.\n","\n","Suppose we want to apply Q-learning to this MDP. Can you think of a way to leverage this offline data to improve the sample-efficiency of the algorithm? What if we were using SARSA instead?"]},{"cell_type":"markdown","metadata":{"id":"mbYTKetHOYU_"},"source":["### **Answer**\n","\n","[your answer here]"]},{"cell_type":"markdown","metadata":{"id":"542QxKsSOs21"},"source":["# Part 3 - RL with Function Approximation"]},{"cell_type":"markdown","metadata":{"id":"OiGZBiJ4PiIE"},"source":["## Question 3.1\n","\n","Given a datset $(s_i, a_i, r_i, s_i')$ of (states, actions, rewards, next states), the Fitted Q-Iteration (FQI) algorithm proceeds as follows:\n","\n","\n","* We start from a $Q$ function $Q_0 \\in \\mathcal{F}$, where $\\mathcal{F}$ is a function space;\n","* At every iteration $k$, we compute $Q_{k+1}$ as:\n","\n","$$\n","Q_{k+1}\\in\\arg\\min_{f\\in\\mathcal{F}} \\frac{1}{2}\\sum_{i=1}^N\n","\\left(\n","  f(s_i, a_i) - y_i^k\n","\\right)^2 + \\lambda \\Omega(f)\n","$$\n","where $y_i^k = r_i + \\gamma \\max_{a'}Q_k(s_i', a')$, $\\Omega(f)$ is a regularization term and $\\lambda > 0$ is the regularization coefficient.\n","\n","\n","Consider FQI with *linear* function approximation. That is, for a given feature map $\\phi : S \\rightarrow \\mathbb{R}^d$, we consider a parametric family of $Q$ functions $Q_\\theta(s,a) = \\phi(s)^T\\theta_a$ for $\\theta_a\\in\\mathbb{R}^d$. Suppose we are applying FQI on a given dataset of $N$ tuples of the form $(s_i, a_i, r_i, s_i')$ and we are at the $k$-th iteration. Let $\\theta_k \\in\\mathbb{R}^{d \\times A}$ be our current parameter. Derive the *closed-form* update to find $\\theta_{k+1}$, using $\\frac{1}{2}\\sum_a ||\\theta_a||_2^2$ as regularization."]},{"cell_type":"markdown","metadata":{"id":"7jx7aE41DkEM"},"source":["### **Answer**\n","\n","[your derivation here]"]},{"cell_type":"markdown","metadata":{"id":"ewHzjm7MVGBg"},"source":["## Question 3.2\n","\n","The code below creates a larger gridworld (with more states than the one used in the previous questions), and defines a feature map. Implement linear FQI to this environment (in the function `linear_fqi()` below), and compare the approximated $Q$ function to the optimal $Q$ function computed with value iteration.\n","\n","Can you improve the feature map in order to reduce the approximation error?"]},{"cell_type":"markdown","metadata":{"id":"Tu4g-HSnEcBs"},"source":["### **Answer**\n","\n","[explanation about how you tried to reduce the approximation error + FQI implementation below]"]},{"cell_type":"code","metadata":{"id":"ZovF3VXOVfCs"},"source":["def get_large_gridworld():\n","  \"\"\"Creates an instance of a grid-world MDP with more states.\"\"\"\n","  walls = [(ii, 10) for ii in range(15) if (ii != 7 and ii != 8)]\n","  env = GridWorld(\n","      nrows=15,\n","      ncols=15,\n","      reward_at = {(14, 14):1.0},\n","      walls=tuple(walls),\n","      success_probability=0.9,\n","      terminal_states=((14, 14),)\n","  )\n","  return env\n","\n","\n","class GridWorldFeatureMap:\n","  \"\"\"Create features for state-action pairs\n","  \n","  Args:\n","    dim: int\n","      Feature dimension\n","    sigma: float\n","      RBF kernel bandwidth\n","  \"\"\"\n","  def __init__(self, env, dim=15, sigma=0.25):\n","    self.index2coord = env.index2coord\n","    self.n_states = env.Ns\n","    self.n_actions = env.Na\n","    self.dim = dim\n","    self.sigma = sigma\n","\n","    n_rows = env.nrows\n","    n_cols = env.ncols\n","\n","    # build similarity matrix\n","    sim_matrix = np.zeros((self.n_states, self.n_states))\n","    for ii in range(self.n_states):\n","        row_ii, col_ii = self.index2coord[ii]\n","        x_ii = row_ii / n_rows\n","        y_ii = col_ii / n_cols\n","        for jj in range(self.n_states):\n","            row_jj, col_jj = self.index2coord[jj]\n","            x_jj = row_jj / n_rows\n","            y_jj = col_jj / n_cols\n","            dist = np.sqrt((x_jj - x_ii) ** 2.0 + (y_jj - y_ii) ** 2.0)\n","            sim_matrix[ii, jj] = np.exp(-(dist / sigma) ** 2.0)\n","\n","    # factorize similarity matrix to obtain features\n","    uu, ss, vh = np.linalg.svd(sim_matrix, hermitian=True)\n","    self.feats = vh[:dim, :]\n","\n","  def map(self, observation):\n","    feat = self.feats[:, observation].copy()\n","    return feat"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"InCfu7F9-TbS"},"source":["env = get_large_gridworld()\n","feat_map = GridWorldFeatureMap(env)\n","\n","# Visualize large gridworld\n","render_policy(env)\n","\n","# The features have dimension (feature_dim).\n","feature_example = feat_map.map(1) # feature representation of s=1\n","print(feature_example)\n","\n","# Initial vector theta representing the Q function\n","theta = np.zeros((feat_map.dim, env.action_space.n))\n","print(theta.shape)\n","print(feature_example @ theta) # approximation of Q(s=1, a)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p21KMmruugO1"},"source":["def linear_fqi(env, feat_map, num_iterations, lambd=0.1, gamma=0.95):\n","  \"\"\"\n","  # Linear FQI implementation\n","  # TO BE COMPLETED\n","  \"\"\"\n","\n","  # get a dataset\n","  dataset = get_uniform_dataset(env, n_samples=400)\n","  #dataset = get_random_policy_dataset(env, n_samples=400)\n","  (states, actions, rewards, nextStates) = dataset \n","\n","  x = 0\n","  while max(rewards) == 0 and x < 20  : \n","    dataset = get_random_policy_dataset(env, n_samples=500)\n","    (states, actions, rewards, nextStates) = dataset \n","    x +=1\n","  print(x)\n","\n","  Rmax = max(rewards)\n","  theta = np.zeros((feat_map.dim, env.Na)) \n","\n","  for it in range(num_iterations):\n","    for action in range(env.Na):\n","      temp = np.copy(theta) \n","      to_inv = 0 \n","      sum_term = 0\n","      for j in range(len(actions)):\n","        if  actions[j] == action :\n","          to_inv += np.outer(feat_map.map(states[j]), feat_map.map(states[j])) \n","          fsa = max( feat_map.map(nextStates[j]) @ temp )\n","          if fsa > 0 :\n","            fsa = min(fsa , Rmax / (1-gamma) )\n","          else : \n","            fsa = max(fsa, - Rmax/(1-gamma))\n","\n","          y_i_k = rewards[j] + gamma * fsa\n","          sum_term += y_i_k * feat_map.map(states[j]) \n","\n","      to_inv += lambd * np.eye(feat_map.dim)\n","      theta[:,action] =  np.linalg.inv(to_inv) @ sum_term \n","  return theta\n","\n","# ----------------------------\n","# Environment and feature map\n","# ----------------------------\n","env = get_large_gridworld()\n","# you can change the parameters of the feature map, and even try other maps!\n","feat_map = GridWorldFeatureMap(env, dim=15, sigma=0.25)\n","\n","# -------\n","# Run FQI\n","# -------\n","theta = linear_fqi(env, feat_map, num_iterations=100)\n","# Compute and run greedy policy\n","Q_fqi = np.zeros((env.Ns, env.Na))\n","for x in range(env.Ns):\n","  state_feat = feat_map.map(x)\n","  Q_fqi[x, :] = state_feat @ theta\n","\n","V_fqi = Q_fqi.max(axis=1)\n","policy_fqi = Q_fqi.argmax(axis=1)\n","render_policy(env, policy_fqi, horizon=100)\n","\n","# Visualize the approximate value function in the gridworld.\n","img = env.get_layout_img(V_fqi)    \n","plt.imshow(img)\n","plt.show()"],"execution_count":null,"outputs":[]}]}
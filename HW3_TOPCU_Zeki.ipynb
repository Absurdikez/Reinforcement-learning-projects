{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MVA - Homework 3 - Reinforcement Learning (2022/2023)\n",
        "\n",
        "**Name:** TOPÃ‡U Zeki\n",
        "\n",
        "**Email:**    zekitopccu@gmail.com\n",
        "\n",
        "\n",
        "Note that the assignment is composed by two parts. This one is the practical part."
      ],
      "metadata": {
        "id": "PklL_7_Wn6Bz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instructions\n",
        "\n",
        "* The deadline is **January 20 (2023) at 11:59 pm (Paris time).**\n",
        "\n",
        "* By doing this homework you agree to the late day policy, collaboration and misconduct rules reported on [Piazza](https://piazza.com/class/l4y5ubadwj64mb/post/6).\n",
        "\n",
        "* **Mysterious or unsupported answers will not receive full credit**. A correct answer, unsupported by calculations, explanation, or algebraic work will receive no credit; an incorrect answer supported by substantially correct calculations and explanations might still receive partial credit.\n",
        "\n",
        "* Answers should be provided in **English**."
      ],
      "metadata": {
        "id": "5n4fvDATsjPj"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrQ-yXS-k0Pm"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Any, Optional, Dict, Tuple\n",
        "from collections import deque, defaultdict\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import normalize as normalize_matrix\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEL2pObSzV9Q"
      },
      "source": [
        "# Problem\n",
        "\n",
        "In this exercise, we investigate the performance of LinUCB. In particular, we want to understand the impact of the representation on the learning process. \n",
        "\n",
        "A representation is a mapping $\\phi_i : S \\times A \\to \\mathbb{R}^{d_i}$ where $S$ is the context space and $A$ is the action space. A representation is **realizable** when $\\exists \\theta \\in \\mathbb{R}^{d_i}$ such that $r(s,a) = \\phi_i(s,a)^\\top \\theta$, for all $s,a$.\n",
        "\n",
        "Note that a linear contextual bandit problem admits multiple realizable representations. The question we want to investigate is: \n",
        "\n",
        "*Are all the representations equally good for learning?*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8mzv1xm2wyQ"
      },
      "source": [
        "### Environment\n",
        "We start definining utility functions and the environment we are going to use. \n",
        "\n",
        "In particular, `LinearEnv` defines a contextual linear bandit problem with stochastic context selection $s_t \\sim \\rho$.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFLJdp8DlWpC"
      },
      "source": [
        "#@title Utilities for building linear representations {display-mode: \"form\"}\n",
        "def normalize_linrep(features, param, scale=1.):\n",
        "    param_norm = np.linalg.norm(param)\n",
        "    new_param = param / param_norm * scale\n",
        "    new_features = features * param_norm / scale\n",
        "    return new_features, new_param\n",
        "\n",
        "def random_transform(features, param, normalize=True, seed=0):\n",
        "    rng = np.random.RandomState(seed)\n",
        "    dim = len(param)\n",
        "    A = rng.normal(size=(dim, dim))\n",
        "\n",
        "    A = rng.normal(size=(dim, dim))\n",
        "    q, r = np.linalg.qr(A)\n",
        "    \n",
        "    new_features = features @ q\n",
        "    new_param = q.T @ param\n",
        "        \n",
        "    if normalize:\n",
        "        new_features, new_param = normalize_linrep(new_features, new_param)\n",
        "    \n",
        "    val = features @ param - new_features @ new_param\n",
        "    assert np.allclose(features @ param, new_features @ new_param)\n",
        "    return new_features, new_param\n",
        "\n",
        "def make_random_linrep(\n",
        "    n_contexts, n_actions, feature_dim, \n",
        "    ortho=True, normalize=True, seed=0,\n",
        "    method=\"gaussian\"):\n",
        "\n",
        "    rng = np.random.RandomState(seed)\n",
        "    if method == \"gaussian\":\n",
        "        features = rng.normal(size=(n_contexts, n_actions, feature_dim))\n",
        "    elif method == \"bernoulli\":\n",
        "        features = rng.binomial(n=1, p=rng.rand(), size=(n_contexts, n_actions, feature_dim))\n",
        "\n",
        "    param = 2 * rng.uniform(size=feature_dim) - 1\n",
        "    \n",
        "    #Orthogonalize features\n",
        "    if ortho:\n",
        "        features = np.reshape(features, (n_contexts * n_actions, feature_dim))\n",
        "        orthogonalizer = PCA(n_components=feature_dim, random_state=seed) #no dimensionality reduction\n",
        "        features = orthogonalizer.fit_transform(features)\n",
        "        features = np.reshape(features, (n_contexts, n_actions, feature_dim))\n",
        "        features = np.take(features, rng.permutation(feature_dim), axis=2)\n",
        "    \n",
        "    if normalize:\n",
        "        features, param = normalize_linrep(features, param)\n",
        "        \n",
        "    return features, param\n",
        "\n",
        "def derank_hls(features, param, newrank=1, transform=True, normalize=True, seed=0):\n",
        "    nc = features.shape[0]\n",
        "\n",
        "    rewards = features @ param\n",
        "    # compute optimal arms\n",
        "    opt_arms = np.argmax(rewards, axis=1)\n",
        "    # compute features of optimal arms\n",
        "    opt_feats = features[np.arange(nc), opt_arms, :]\n",
        "    opt_rews = rewards[np.arange(nc), opt_arms].reshape((nc, 1)) \n",
        "    remove = min(max(nc - newrank + 1, 0), nc)\n",
        "    \n",
        "    new_features = np.array(features)\n",
        "    outer = np.matmul(opt_rews[:remove], opt_rews[:remove].T)\n",
        "    xx = np.matmul(outer, opt_feats[:remove, :]) \\\n",
        "        / np.linalg.norm(opt_rews[:remove])**2\n",
        "    new_features[np.arange(remove), opt_arms[:remove], :] = xx\n",
        "    \n",
        "    new_param = param.copy()\n",
        "    \n",
        "    if transform:\n",
        "        new_features, new_param = random_transform(new_features, new_param, normalize=normalize, seed=seed)\n",
        "    elif normalize:\n",
        "        new_features, new_param = normalize_linrep(new_features, new_param, seed=seed)\n",
        "        \n",
        "    assert np.allclose(features @ param, new_features @ new_param)\n",
        "    return new_features, new_param\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elthSJgamhEj"
      },
      "source": [
        "class LinearEnv():\n",
        "    def __init__(self, features, param, rew_noise=0.5, random_state=0) -> None:\n",
        "        self.features = features\n",
        "        self.param = param\n",
        "        self.rewards = features @ param\n",
        "        self.rew_noise = rew_noise\n",
        "        self.random_state = random_state\n",
        "        self.rng = np.random.RandomState(random_state)\n",
        "        self.n_contexts, self.n_actions, self.feat_dim = self.features.shape\n",
        "\n",
        "    def get_available_actions(self):\n",
        "        \"\"\" Return the actions available at each time\n",
        "        \"\"\"\n",
        "        actions = np.arange(self.n_actions)\n",
        "        return actions\n",
        "    \n",
        "    def sample_context(self):\n",
        "        \"\"\" Return a random context\n",
        "        \"\"\"\n",
        "        self.idx = self.rng.choice(self.n_contexts, 1).item()\n",
        "        return self.idx\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\" Return a realization of the reward in the context for the selected action\n",
        "        \"\"\"\n",
        "        return self.rewards[self.idx, action] + self.rng.randn() * self.rew_noise\n",
        "\n",
        "    def best_reward(self):\n",
        "        \"\"\" Maximum reward in the current context\n",
        "        \"\"\"\n",
        "        return self.rewards[self.idx].max()\n",
        "    \n",
        "    def expected_reward(self, action):\n",
        "        return self.rewards[self.idx, action]\n",
        "\n",
        "class LinearRepresentation():\n",
        "    \"\"\" Returns the features associated to each context and action\n",
        "    \"\"\"\n",
        "    def __init__(self, features) -> None:\n",
        "        self.features = features\n",
        "    \n",
        "    def features_dim(self):\n",
        "        return self.features.shape[2]\n",
        "    \n",
        "    def get_features(self, context, action):\n",
        "        return self.features[context, action]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definition of the environment and example of interaction loop."
      ],
      "metadata": {
        "id": "Zd8LoQzUpKkR"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTdE7MhtlPtH"
      },
      "source": [
        "SEED = 0\n",
        "NOISE = 0.5\n",
        "nc, na, dim = 100, 5, 10\n",
        "features, param = make_random_linrep(\n",
        "    n_contexts=nc, n_actions=na, feature_dim=dim, \n",
        "    ortho=True, normalize=True, seed=SEED, method=\"gaussian\")\n",
        "\n",
        "env = LinearEnv(features=features, param=param, rew_noise=NOISE)\n",
        "for t in range(10):\n",
        "    context = env.sample_context()\n",
        "    avail_actions = env.get_available_actions()\n",
        "    # random action selection\n",
        "    action = np.random.choice(avail_actions, 1).item()\n",
        "    reward = env.step(action)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jp4smEDiowQZ"
      },
      "source": [
        "# Step 1: LinUCB with different representations\n",
        "Implement and test LinUCB with different representations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LBHeFvconxY"
      },
      "source": [
        "class LinUCB:\n",
        "  \n",
        "    def __init__(self, \n",
        "                 env, representation, reg_val, noise_std,\n",
        "                 features_bound,\n",
        "                 param_bound, delta=0.01, random_state=0):\n",
        "        self.env = env\n",
        "        self.rep = representation # linear representation used by LinUCB\n",
        "        self.reg_val = reg_val\n",
        "        self.noise_std = noise_std # noise standard deviation\n",
        "        self.features_bound = features_bound # bound on the features\n",
        "        self.param_bound=param_bound # bound on the parameter\n",
        "        self.delta = delta\n",
        "        self.random_state = random_state\n",
        "        self.rng = np.random.RandomState(random_state)\n",
        "\n",
        "    def run(self, horizon):\n",
        "        instant_reward = np.zeros(horizon)\n",
        "        best_reward = np.zeros(horizon)\n",
        "        \n",
        "        dim = self.rep.features_dim()\n",
        "        # Initialize required variables\n",
        "        #--------------------------------------------\n",
        "        # TODO\n",
        "        # Using same notation as in the course slides\n",
        "        # V^-1\n",
        "        b_vec =  np.zeros(dim)\n",
        "        inv_v = 1/ self.reg_val * np.eye(dim)       \n",
        "        #--------------------------------------------\n",
        "\n",
        "        for t in range(horizon):\n",
        "            context = env.sample_context()\n",
        "            avail_actions = env.get_available_actions()\n",
        "            # Implement the optimistic action selection\n",
        "            #--------------------------------------------\n",
        "            # TODO\n",
        "            beta = self.noise_std * np.sqrt(\n",
        "                    dim * np.log((1 + t*self.features_bound**2/ self.reg_val\n",
        "                             )/ self.delta)\n",
        "                    ) + np.sqrt(self.reg_val) * self.param_bound\n",
        "\n",
        "            # UCB evaluation\n",
        "            maxUCB = -np.inf\n",
        "            BestAction = avail_actions[0]\n",
        "            theta = inv_v @ b_vec\n",
        "            for action in avail_actions:\n",
        "                phi = self.rep.get_features(context, action)\n",
        "                UCB = np.dot(phi, theta) + beta * np.sqrt(np.dot(phi, np.dot(inv_v, phi)))\n",
        "                if UCB > maxUCB:\n",
        "                    BestAction = action\n",
        "                    maxUCB = UCB              \n",
        "            action = BestAction\n",
        "            #--------------------------------------------\n",
        "            \n",
        "            # execute action\n",
        "            reward = env.step(action)\n",
        "\n",
        "            # get features corresponding to the selected action\n",
        "            v = self.rep.get_features(context, action)\n",
        "\n",
        "            # update internal model\n",
        "            #--------------------------------------------\n",
        "            # TODO\n",
        "            b_vec += reward * v\n",
        "            inv_v -= inv_v @ np.outer(v, v)  @ inv_v / (1 + v.T @ inv_v @ v)\n",
        "            #--------------------------------------------\n",
        "\n",
        "            # regret computation\n",
        "            instant_reward[t] = self.env.expected_reward(action)\n",
        "            best_reward[t] = self.env.best_reward()\n",
        "        \n",
        "        # define the regret\n",
        "        #--------------------------------------------\n",
        "        regret = [np.sum(best_reward[:t] - instant_reward[:t]) for t in range(horizon)]\n",
        "        #--------------------------------------------\n",
        "        return {\"regret\": regret}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the algorithm"
      ],
      "metadata": {
        "id": "CyWRzqWE5QFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 0\n",
        "NOISE = 0.5\n",
        "nc, na, dim = 100, 5, 10\n",
        "features, param = make_random_linrep(\n",
        "    n_contexts=nc, n_actions=na, feature_dim=dim, \n",
        "    ortho=True, normalize=True, seed=SEED, method=\"gaussian\")\n",
        "\n",
        "env = LinearEnv(features=features, param=param, rew_noise=NOISE)"
      ],
      "metadata": {
        "id": "TGq2Cu6W5WmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rN5QgzhZq32w"
      },
      "source": [
        "T=10000\n",
        "NRUNS = 2\n",
        "rep = LinearRepresentation(env.features)\n",
        "\n",
        "regrets = np.zeros((NRUNS,T))\n",
        "for r in range(NRUNS):\n",
        "    algo = LinUCB(\n",
        "        env, representation=rep, reg_val=1, \n",
        "        noise_std=NOISE, \n",
        "        features_bound=np.linalg.norm(env.param,2), \n",
        "        param_bound=np.linalg.norm(env.features,2, axis=-1).max()\n",
        "    )\n",
        "    output = algo.run(T)\n",
        "    regrets[r] = output['regret']\n",
        "\n",
        "mr = np.mean(regrets, axis=0)\n",
        "vr = np.std(regrets, axis=0) / np.sqrt(NRUNS)\n",
        "plt.plot(np.arange(T), mr)\n",
        "plt.fill_between(np.arange(T), mr - 2*vr, mr + 2*vr, alpha=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can construct equivalent representations with the *same size*. \n",
        "\n",
        "We already provide the code for building such representations."
      ],
      "metadata": {
        "id": "KR78xckV5Z5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rep_list = []\n",
        "param_list = []\n",
        "for i in range(1, dim):\n",
        "    fi, pi = derank_hls(features=features, param=param, newrank=i, transform=True, normalize=True, seed=np.random.randint(1, 1234144,1))\n",
        "    rep_list.append(LinearRepresentation(fi))\n",
        "    param_list.append(pi)\n",
        "rep_list.append(LinearRepresentation(features))\n",
        "param_list.append(param)\n",
        "\n",
        "for i in range(len(rep_list)):\n",
        "    print()\n",
        "    print(f\"feature norm({i}): {np.linalg.norm(rep_list[i].features,2,axis=-1).max()}\")\n",
        "    print(f\"param norm({i}): {np.linalg.norm(param_list[i],2)}\")\n",
        "    assert np.allclose(rep_list[i].features @ param_list[i], features @ param) #check that they are all equivalent\n",
        "print()"
      ],
      "metadata": {
        "id": "fmOY0ZTO5aKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's run LinUCB with each representation"
      ],
      "metadata": {
        "id": "DIsp8bQAq_Ce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "T=20000\n",
        "NRUNS = 2\n",
        "\n",
        "results = {}\n",
        "for nr, rep in enumerate(rep_list):\n",
        "    regrets = np.zeros((NRUNS,T))\n",
        "    for r in range(NRUNS):\n",
        "        algo = LinUCB(env, representation=rep, reg_val=1, noise_std=NOISE, \n",
        "                      features_bound=np.linalg.norm(rep.features,2, axis=-1).max(),\n",
        "                      param_bound=np.linalg.norm(param_list[nr],2)\n",
        "                      )\n",
        "        output = algo.run(T)\n",
        "        regrets[r] = output['regret']\n",
        "\n",
        "    mr = np.mean(regrets, axis=0)\n",
        "    vr = np.std(regrets, axis=0) / np.sqrt(NRUNS)\n",
        "    results[f'LinUCB-rep_{nr}'] = {'regret': mr, \"std\": vr}"
      ],
      "metadata": {
        "id": "GCt-zyC35u4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,8))\n",
        "for k,v in results.items():\n",
        "    plt.plot(np.arange(T), v['regret'], label=k)\n",
        "    plt.fill_between(np.arange(T), v['regret'] - 2*v['std'], v['regret'] + 2*v['std'], alpha=0.2)\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "kRpR5TPB8kof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If everything is implemented correctly, there is a representation with a much better regret. \n",
        "\n",
        "**Q1**: Why? What is the property of such a representation?\n",
        "\n",
        "See [Leveraging Good Representations in Linear Contextual Bandits](https://arxiv.org/abs/2104.03781) for the answer."
      ],
      "metadata": {
        "id": "3iuRWV_16Lk2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**write answer here**\n",
        "\n",
        "A representation achieves constant regret if and only if it satisfies HLS condition. Obviously, representation 9 is the only one who satisfies it. By observing so, we can unsure that algorithm collected enough information about the optimal parameter $\\theta^*$."
      ],
      "metadata": {
        "id": "fcfhssc364q2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEKKBmV1twEW"
      },
      "source": [
        "# Step 2: representation selection"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have seen that not all the representations are equal, we want to design an algorithm able to leverage the most efficient representation when provided with a set of **realizable** representations.\n",
        "\n",
        "\n",
        "This algorithm exists and is called LEADER. Implement the LEADER algorithm as reported in the paper [\"Leveraging Good Representations in Linear Contextual Bandits\"](https://arxiv.org/abs/2104.03781)."
      ],
      "metadata": {
        "id": "EQM_QsHh6881"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLKGjbGprSKa"
      },
      "source": [
        "class LEADER:\n",
        "  \n",
        "    def __init__(self, \n",
        "                 env, representations, reg_val, noise_std,\n",
        "                 features_bounds,\n",
        "                 param_bounds, delta=0.01, random_state=0\n",
        "        ):\n",
        "        self.env = env\n",
        "        self.reps = representations #list of representations\n",
        "        self.reg_val = reg_val\n",
        "        self.noise_std = noise_std\n",
        "        self.features_bound = features_bounds #list of feature bounds\n",
        "        self.param_bound=param_bounds #list of parameter bounds\n",
        "        self.delta = delta\n",
        "        self.random_state = random_state\n",
        "        self.rng = np.random.RandomState(random_state)\n",
        "\n",
        "    def run(self, horizon):\n",
        "        instant_reward = np.zeros(horizon)\n",
        "        best_reward = np.zeros(horizon)\n",
        "        M = len(self.reps)\n",
        "        \n",
        "        inv_A = []\n",
        "        b_vec = []\n",
        "        A_logdet = []\n",
        "        theta = []\n",
        "        for i in range(M):\n",
        "            dim = self.reps[i].features_dim()\n",
        "            \n",
        "            # Initialize required variables\n",
        "            #--------------------------------------------\n",
        "            # TODO\n",
        "            b_vec.append(np.zeros(dim))\n",
        "            theta.append(np.zeros(dim))\n",
        "            inv_A.append(1 / self.reg_val * np.eye(dim))           \n",
        "            A_logdet.append(np.log(np.linalg.det(self.reg_val * np.eye(dim))))\n",
        "            \n",
        "            #--------------------------------------------\n",
        "        \n",
        "        for t in range(horizon):\n",
        "            context = env.sample_context()\n",
        "            avail_actions = env.get_available_actions()\n",
        "\n",
        "            # Implement the action selection strategy\n",
        "            #--------------------------------------------\n",
        "            # TODO\n",
        "            UCBs = np.empty([M, len(avail_actions)])\n",
        "            for i in range(M):\n",
        "                dim = self.reps[i].features_dim()\n",
        "                beta = self.noise_std * np.sqrt(\n",
        "                  A_logdet[i] - np.log(np.linalg.det(self.reg_val * np.eye(dim)))\n",
        "                    -2 * np.log(self.delta)) + np.sqrt(self.reg_val) * self.param_bound[i]\n",
        "                for j, action in enumerate(avail_actions):\n",
        "                    x = self.reps[i].get_features(context, action)\n",
        "                    UCBs[i, j] = np.dot(x, theta[i])+ beta* np.sqrt(np.dot(x, np.dot(x, inv_A[i])))\n",
        "            \n",
        "            action = avail_actions[np.argmax(UCBs.min(axis=0))]\n",
        "            #--------------------------------------------\n",
        "            \n",
        "            #execute action\n",
        "            reward = env.step(action)\n",
        "\n",
        "            # update\n",
        "            for j in range(M):\n",
        "                v = self.reps[j].get_features(context, action)\n",
        "                # update internal model\n",
        "                #--------------------------------------------\n",
        "                # TODO\n",
        "                inv_A[j] -= inv_A[j] @ np.outer(v,v)  @ inv_A[j]/(1+v.T @ inv_A[j] @ v)\n",
        "                b_vec[j] += reward*v\n",
        "                A_logdet[j] = np.log(1/ np.linalg.det(inv_A[j]))\n",
        "                theta[j] = inv_A[j] @ b_vec[j]\n",
        "\n",
        "                #--------------------------------------------\n",
        "\n",
        "            # regret computation\n",
        "            instant_reward[t] = self.env.expected_reward(action)\n",
        "            best_reward[t] = self.env.best_reward()\n",
        "        \n",
        "        \n",
        "        # define the regret\n",
        "        #--------------------------------------------\n",
        "        regret = [np.sum(best_reward[:t] - instant_reward[:t]) for t in range(horizon)]\n",
        "        #--------------------------------------------\n",
        "        return {\"regret\": regret}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5YAbauzt3Ho"
      },
      "source": [
        "regrets = np.zeros((NRUNS,T))\n",
        "M = len(rep_list)\n",
        "for r in range(NRUNS):\n",
        "    algo = LEADER(env, representations=rep_list, reg_val=1, noise_std=NOISE, \n",
        "                  features_bounds=[np.linalg.norm(rep_list[j].features,2, axis=-1).max() for j in range(M)], \n",
        "                  param_bounds=[np.linalg.norm(param_list[j],2) for j in range(M)]\n",
        "    )\n",
        "    output = algo.run(T)\n",
        "    regrets[r] = output['regret']\n",
        "\n",
        "mr = np.mean(regrets, axis=0)\n",
        "vr = np.std(regrets, axis=0) / np.sqrt(NRUNS)\n",
        "results['LEADER'] = {'regret': mr, 'std': vr}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwtWqSUbvv4t"
      },
      "source": [
        "plt.figure(figsize=(15,8))\n",
        "for k,v in results.items():\n",
        "    if k == 'LEADER':\n",
        "      plt.plot(np.arange(T), v['regret'], '--', label=k)\n",
        "    else:\n",
        "      plt.plot(np.arange(T), v['regret'], label=k)\n",
        "    plt.fill_between(np.arange(T), v['regret'] - 2*v['std'], v['regret'] + 2*v['std'], alpha=0.2)\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If correctly implemented, LEADER should have the second best performance\n",
        "\n",
        "**Q2**: so far we have considered only *realizable* representations, i.e., $\\forall i, \\exists \\theta \\in \\mathbb{R}^{d_i}$ such that $r(s,a) = \\phi_i(s,a)^\\top \\theta$, for all $s,a$. Now suppose that this property holds only for a single representation $i^\\star$, while for all $i\\neq i^\\star$ we have $\\forall \\theta \\in \\mathbb{R}^{d_i} \\exists s,a$ such that $r(s,a) \\neq \\phi_i(s,a)^\\top \\theta$. Do you think the LEADER algorithm would still work (i.e., achieve sub-linear regret) for this setting? Why?"
      ],
      "metadata": {
        "id": "0k8-5kMW8vjW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ANSWER TO Q2**\n",
        "If we consider the Equation (1) of section 5.1, one can notice that, if we can't find a theta for which $<\\phi(s,a) | \\theta>$ = r(s,a), the inequality does not hold. Since LEADER is based on this inequality, then we have no guarantee that the algorithm will select the optimal action.\n",
        "\n"
      ],
      "metadata": {
        "id": "ThzmDxKJlOhh"
      }
    }
  ]
}
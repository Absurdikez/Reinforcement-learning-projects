{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vubEiQiK90oJ"
      },
      "source": [
        "# MVA - Homework 2 - Reinforcement Learning (2022/2023)\n",
        "\n",
        "**Name:** TOPCU Zeki\n",
        "**Email:**    zekitopccu@gmail.comm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfO5pnmP95Gj"
      },
      "source": [
        "## Instructions\n",
        "\n",
        "* The deadline is **December 16 (2022) at 11:59 pm (Paris time).**\n",
        "\n",
        "* By doing this homework you agree to the late day policy, collaboration and misconduct rules reported on [Piazza](https://piazza.com/class/l4y5ubadwj64mb?cid=6).\n",
        "\n",
        "* **Mysterious or unsupported answers will not receive full credit**. A correct answer, unsupported by calculations, explanation, or algebraic work will receive no credit; an incorrect answer supported by substantially correct calculations and explanations might still receive partial credit.\n",
        "\n",
        "* Answers should be provided in **English**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nu9nTpYZ-FSq"
      },
      "source": [
        "# Colab setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdwHE3b--HvV"
      },
      "source": [
        "from IPython import get_ipython\n",
        "\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "  # install rlberry library\n",
        "  !pip install git+https://github.com/rlberry-py/rlberry.git@mva2021#egg=rlberry[default] > /dev/null 2>&1\n",
        "\n",
        "  # pytorch\n",
        "  !pip install torch > /dev/null 2>&1\n",
        "\n",
        "  # install ffmpeg-python for saving videos\n",
        "  !pip install ffmpeg-python > /dev/null 2>&1\n",
        "\n",
        "  # packages required to show video\n",
        "  !pip install pyvirtualdisplay > /dev/null 2>&1\n",
        "  !apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "  !pip uninstall gym\n",
        "  !pip install gym==0.22.0\n",
        "\n",
        "  print(\"Libraries installed, please restart the runtime!\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSG1D122-pA8"
      },
      "source": [
        "# Create directory for saving videos\n",
        "!mkdir videos > /dev/null 2>&1\n",
        "!mkdir Xvfb\n",
        "# Initialize display and import function to show videos\n",
        "import rlberry.colab_utils.display_setup\n",
        "from rlberry.colab_utils.display_setup import show_video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5E1CUDDz-qhc"
      },
      "source": [
        "# Useful imports\n",
        "import gym\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from copy import deepcopy\n",
        "from gym.wrappers import Monitor\n",
        "from rlberry.agents import Agent\n",
        "from rlberry.manager import AgentManager, plot_writer_data\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "# torch device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTFemQorBP-8"
      },
      "source": [
        "# Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mf-ItKXi_we7"
      },
      "source": [
        "## Environment\n",
        "In the coding exercises, you will use the CartPole environment from [Gym](https://gym.openai.com/) library. The cells below show how to interact with this MDP and how to visualize it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYGK3X9JBPUi"
      },
      "source": [
        "def get_env():\n",
        "  \"\"\"Creates an instance of a CartPole-v0 environment.\"\"\"\n",
        "  return gym.make('CartPole-v0')\n",
        "\n",
        "def render_policy(env, agent):\n",
        "  env = deepcopy(env)\n",
        "  env = Monitor(env, './videos', force=True, video_callable=lambda episode: True)\n",
        "  for episode in range(1):\n",
        "    done = False\n",
        "    state = env.reset()\n",
        "    env.render()\n",
        "    while not done:\n",
        "        action = agent.select_action(state, evaluation=True)\n",
        "        state, reward, done, info = env.step(action)\n",
        "        env.render()\n",
        "    env.close()\n",
        "    show_video()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDT9RthO-_X9"
      },
      "source": [
        "## Running experiments with `rlberry`\n",
        "In order to compare different algorithms, you'll need to run experiments several times and plot the results. For that, we use the [rlberry](https://github.com/rlberry-py/rlberry) library here.\n",
        "The code below gives a quick introduction to what you will need from this library, but you can also check its documentation [here](https://rlberry.readthedocs.io/en/latest/).\n",
        "\n",
        "Basically, all you have to do is to write your agents using the `rlberry.agents.Agent` interface, and `rlberry` provides an `AgentManger` class that allows you to run your agents in parallel and plot the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aTH9MT6_1Yy"
      },
      "source": [
        "from rlberry.agents import Agent\n",
        "\n",
        "class MyAgent(Agent):\n",
        "  name = \"MyAgent\"\n",
        "  def __init__(self, env, param1, param2, **kwargs):\n",
        "    \"\"\"\n",
        "    The base class (Agent) initializes:\n",
        "      self.env : instance of the environment used for training (in fit() method)\n",
        "      self.eval_env : instance of the environment used for evaluation (in eval() method)\n",
        "      self.rng : random number generator (https://numpy.org/doc/stable/reference/random/generator.html)\n",
        "      self.writer : use self.writer.add_scalar(tag, value, global_step) to log training data\n",
        "\n",
        "    For reproducibility, use ONLY self.rng if you need random numbers in you agent!\n",
        "    To be able to visualize plots with AgentManager, log data using self.writer (see below)\n",
        "    \"\"\"\n",
        "    Agent.__init__(self, env, **kwargs)\n",
        "    self.param1 = param1\n",
        "    self.param2 = param2\n",
        "    self.total_steps = 0\n",
        "    self.total_episodes = 0\n",
        "\n",
        "  def select_action(self, state, evaluation=False):\n",
        "    \"\"\"\n",
        "    If evaluation=True, run evaluation policy (e.g., greedy with respect to Q)\n",
        "    If evaluation=False, run exploration policy (e.g., epsilon greedy)\n",
        "    \"\"\"\n",
        "    return self.env.action_space.sample()  # random action for this example\n",
        "\n",
        "  def fit(self, budget):\n",
        "    \"\"\"budget = number of timesteps to train your agent\"\"\"\n",
        "    state = self.env.reset()\n",
        "    episode_reward = 0.0\n",
        "    for tt in range(budget):\n",
        "      self.total_steps += 1\n",
        "      action = self.select_action(state, evaluation=False)\n",
        "      next_state, reward, done, _ = self.env.step(action)\n",
        "      episode_reward += reward\n",
        "\n",
        "      # Log data\n",
        "      self.writer.add_scalar('rewards', reward, global_step=self.total_steps)\n",
        "\n",
        "      state = next_state\n",
        "      if done:\n",
        "        self.total_episodes += 1\n",
        "        # Log episode data\n",
        "        self.writer.add_scalar('episode_rewards', episode_reward, global_step=self.total_steps)\n",
        "        self.writer.add_scalar('episode', self.total_episodes, global_step=self.total_steps)\n",
        "\n",
        "        state = self.env.reset()\n",
        "        episode_reward = 0.0\n",
        "\n",
        "\n",
        "  def eval(self, **kwargs):\n",
        "    \"\"\"\n",
        "    Here, you can run Monte-Carlo policy evaluation\n",
        "    with self.eval_env and return the result.\n",
        "    Returning zero for this example.\n",
        "    \"\"\"\n",
        "    return 0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGToS1W-A_21"
      },
      "source": [
        "#\n",
        "# Initialize and train a single instance of MyAgent\n",
        "#\n",
        "my_agent = MyAgent(\n",
        "    env=(get_env, {}),       # tuple (constructor, kwargs)\n",
        "    param1=10,               # extra params your agent might need\n",
        "    param2=15)\n",
        "# train the agent for 100 timesteps\n",
        "my_agent.fit(100)\n",
        "# pandas DataFrame containing data stored with my_agent.writer.add_scalar(tag, value, global_step)\n",
        "print(my_agent.writer.data)\n",
        "\n",
        "#\n",
        "# Run several instances of MyAgent in parallel and plot the results\n",
        "#\n",
        "manager_kwargs = dict(\n",
        "    agent_class=MyAgent,\n",
        "    train_env=(get_env, dict()),\n",
        "    eval_env=(get_env, dict()),\n",
        "    fit_budget=100,                    # Number of total timesteps\n",
        "    n_fit=2,                           # Number of agent instances to fit\n",
        "    parallelization='thread',          # Use 'thread' in the notebook!\n",
        "    seed=456,                          # Seed\n",
        "    default_writer_kwargs=dict(maxlen=None,log_interval=10),\n",
        ")\n",
        "my_agent_manager = AgentManager(\n",
        "    init_kwargs=dict(param1=10, param2=20),\n",
        "    agent_name='MyAgent',\n",
        "    **manager_kwargs\n",
        ")\n",
        "my_agent_manager.fit()   # Train 'n_fit' instances in parallel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0b-fMr-jGvfP"
      },
      "source": [
        "# Plot the results\n",
        "example_managers = []\n",
        "example_managers.append(my_agent_manager)    # You could add more managers here, for other agents/parameters\n",
        "_ = plot_writer_data(example_managers, tag='rewards', title='Instantaneous Rewards')\n",
        "_ = plot_writer_data(example_managers, tag='episode_rewards', xtag='episode', title='Episode Rewards')\n",
        "_ = plot_writer_data(example_managers, tag='episode_rewards', xtag='episode', title='Cumulative Episode Rewards', preprocess_func=np.cumsum)\n",
        "\n",
        "# Render the policy of one of the trained agents\n",
        "agent_instance = my_agent_manager.get_agent_instances()[0]\n",
        "render_policy(agent_instance.eval_env, agent_instance)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFskJZlZlT0N"
      },
      "source": [
        "# Part 1: DQN\n",
        "\n",
        "The goal of this exercise is to compare different variants of Deep Q-Learning (a.k.a. DQN).\n",
        "\n",
        "**DQN.**\n",
        "Recall from the class the DQN aims to minimize the following loss function\n",
        "$$\n",
        "    L(\\theta) = \\mathbb{E}_{s,a,r,s' \\sim \\mathcal{D}} \\left[\n",
        "\\left(\n",
        "    r + \\gamma \\max_{a' \\in\\mathcal{A}} Q(s',a';\\theta') - Q(s,a; \\theta)\n",
        "\\right)^2\n",
        "    \\right ]\n",
        "$$\n",
        "where $\\theta'$ is the parameter of the target network updated every $C$ iterations from $\\theta$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMHOQC7k6pF4"
      },
      "source": [
        "## Question 1.1 (written)\n",
        "\n",
        "**(a)** The DQN objective resembles a classical supervised learning problem. Could you highlight the differences?\n",
        "\n",
        "**(b)** Could you describe the role of $C$ and the trade-off at play in choosing a good value for $C$?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kUG58qkl0lX"
      },
      "source": [
        "####ANSWERS####\n",
        "\n",
        "**(a)** In the classical supervised learning problem, the data set remains constant and does not change during training.\n",
        "\n",
        "On the other hand, in DQN, the dataset evolves during training along with the data collected during interaction with the environment. Since we are bootstrapping the Q function, the target used to train the model is generated by the model itself.\n",
        "\n",
        "**(b)** The main task of the C parameter is to control the stability of the target used. Setting C=1 results in not using a target network and using the same model to generate targets, resulting in unstable training. If C is too small, training will be difficult due to instability. On the other hand, if C is too large, the learned function will not be correct."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5akQ0_LB7Dgu"
      },
      "source": [
        "## Question 2.2 (implementation)\n",
        "\n",
        "We would like to evaluate DQN with newer variants of DQN, namely Double DQN, Dueling DQN and Double Dueling DQN.\n",
        "In class we have seen the principle of DQN, please refer to the following paper for Double and Dueling DQN (https://arxiv.org/abs/1511.06581).\n",
        "\n",
        "\n",
        "\n",
        "The difference between DQN and Double DNQ is how the target network is built. In Double DQN we use the greedy action suggested by $Q(s,a; \\boldsymbol{\\theta})$ to evaluate the target (i.e., $\\theta'$), see the appendix of https://arxiv.org/abs/1511.06581.\n",
        "\n",
        "In Dueling DQN, the Q function is estimated as\n",
        "$$\n",
        "    Q(s,a) = V(s) + A(s,a) - \\frac{1}{|\\mathcal{A}|} \\sum_{a'} A(s,a')\n",
        "$$\n",
        "$V$ and $A$ share parameters.\n",
        "Dueling DQN can be implemented \"standard\" or \"double\".\n",
        "\n",
        "\n",
        "Starting from the provided code:\n",
        "- (code) Implement DQN and Double DQN. Use the network in the figure below **(top)**.\n",
        "- (code) Implement Dueling DQN and Dueling Double DQN. See the figure below **(bottom)** for the network with shared parameters.\n",
        "- Compare the performance of the algorithms.\n",
        "\n",
        "\n",
        "More precisely, you'll have to:\n",
        "* complete the code for the networks `QNet` and `DuelingQNet`;\n",
        "* compute the loss in the fit() method of `DQNAgent`\n",
        "* define the exploration and evaluation policy in the `select_action` method of `DQNAgent`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEtkn8KGBjCY"
      },
      "source": [
        "![rlmva_homework2.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAosAAAEZCAYAAAD/rG/CAAAAAXNSR0IArs4c6QAACLN0RVh0bXhmaWxlACUzQ214ZmlsZSUyMGhvc3QlM0QlMjJhcHAuZGlhZ3JhbXMubmV0JTIyJTIwbW9kaWZpZWQlM0QlMjIyMDIxLTExLTIxVDE5JTNBNDIlM0E1OS44MDZaJTIyJTIwYWdlbnQlM0QlMjI1LjAlMjAoWDExJTNCJTIwTGludXglMjB4ODZfNjQpJTIwQXBwbGVXZWJLaXQlMkY1MzcuMzYlMjAoS0hUTUwlMkMlMjBsaWtlJTIwR2Vja28pJTIwQ2hyb21lJTJGOTYuMC40NjY0LjQ1JTIwU2FmYXJpJTJGNTM3LjM2JTIyJTIwZXRhZyUzRCUyMkFKSmwwdFpkSjFNZEdYbnU3MEd6JTIyJTIwdmVyc2lvbiUzRCUyMjE1LjcuMCUyMiUyMHR5cGUlM0QlMjJkZXZpY2UlMjIlM0UlM0NkaWFncmFtJTIwaWQlM0QlMjJKaW9XYnF2cE5CZmVNSDZXOFY4UyUyMiUyMG5hbWUlM0QlMjJQYWdlLTElMjIlM0U3VnhkYzZNMkZQMDFudGw5U0Fja1BoOFRaejg2czkxdU43UHQ3aU1CR1RPTFVRcEs3UFRYVndTQlFaQ2lZb01ZaXhlUGRBVVhwSE1renIxU3NvTHIzZUZENmoxc2Y4TUJpbGRBQ3c0cmVMc0N3SFYwJTJCcHNibmd1RGFkcUZJVXlqb0REcFI4TmQ5QTlpUm8xWkg2TUFaWTBMQ2NZeGlSNmFSaDhuQ2ZKSnclMkJhbEtkNDNMOXZndVBuVUJ5OUVMY09kNzhWdDYxOVJRTGFGMVFIMjBmNFJSZUcyZkxKdXVVWEx6aXN2Wmk2eXJSZmdmV0Y2NlJ4OHQ0THJGR05TbEhhSE5ZcnpzU3ZIcFJpQjk2JTJCMFZpJTJCV29vU0kzUEQ5YiUyQmRyNlB6RXYzNjdDaiUyRmZKTDklMkZNZTglMkJYekV2VDE3OHlEcThBbFpNJTJGZDNjMDBLWUZ6NUZDZkxTMGt6OVZ5MnNYJTJCUzVIS3dVUHlZQnlwJTJCbjBlYjlOaUxvN3NIejg5WTlaUWUxYmNrdXBqV2RGbE5NUEJMaGhGYXYzUHo2VFJUSGF4emo5TVVaRER6a2JIeHF6MGlLZjZKYWklMkJVNzZINVR2Y0FUU2drNnZEb3FlalhXbEtNSTd4QkpuJTJCa2w3QWFiOFl6UjAyWFYlMkZSRnJuWTJydHEzaFhObzhScSUyQndjbnhFZ0JZWUNQOERFS3Nma0slMkZvMDdmcDRkaHNOc0R2aENPdzdpM1RPZzhjdWprelBHelZKd2lBTTBQRVVYdUdRSDFtZUxpcXp4RG96Z3lSa2lFTlNMaHhSa2x3bmNzald2TmpMOHNpbnglMkZhT2c3b0VKSHZlZmtYazlWJTJCc092eTh1MmhkdG50YzFsSmFGOXFOJTJCWFZIJTJGVzI0MjB2dGZLJTJCNGxWUjBOSmxIQjYwTyUyRmd4OVZHJTJGdWlGZUdpTFM5OUZ0NDFzRDBPekFyN1NsS0tZc2ZHcSUyQmJoZW83QWxmY0VRNzhpcDlvTWJ4b3VnbXU2c3U4RGhIaHRGMFZMMWc2YWdZaDVhakY0NVYzVDZCZGwxaVVrWGFXWUswczJYU2psZGFPcjhjaWRJTzZEMk94cVlkV0doWGw2cTl0SE9rMHM3aTJBS0gwZzcwT0JxYmRuQ2hYVjJQOTlMT2xVbzdMczdYamFHMGd6Mk94cWFkb1FMdFpOR0VYNXkwdkglMkJEZUtMenkxUGIxZGhNTVR1WXdnVm1HWTJmRUwxbVphN2ZaUFRuclZDTVJrTWwwbVJVTTc1S2NJSzRZSXladkRnSzgyak5wM0FqYXIlMkZKQTYlMkZJOSUyQkpyMXJDTGdpQiUyRlRHZmsxJTJCVHQ2b1RJallIaU1LVldJNWZWUVM3JTJCeTNLJTJCUUUwZzJ4ZEVPOXBpR1lvZ1U5N0F4VEpPTzZhZUZpbUJQS0NhU0ZYaDZseVFFc2dQRmtpdEFQVDhQSDJVcVlXWVlmY2lwbXVUUXRhVlFweGMwNGhxRXdGSkxGWHFHcHlJY2R4aENzYmswNXFjbjVIbFM2bWYlMkZtc2U1N0xsbWdvWHNLWWxQOERrclVJYXBvU0pnMXY2QWd3RlJNMWxid2h3NjJzMXNhVnRDRUFCOVhMSmUyWjhybk1HaUhTcGxJc0w0d1UlMkJsVkEwV1E2bFpzdjVUWnJxSTN2cUpvMmhjNDVHJTJGcXpDV1dndFJmSkh0SDZ1JTJGRkdIcTVHWllnZ0lNTVh6UjNRWmtLdTFESUh6ZTJxbUpmZ0VFakFreTJJRDlFTjEyYklZV0dZVGtqSkQzYk0yanliQ2pLNHRQTFVoY1dSRDByVzlwVkNrQW8yNXpSR0JYU1NsQUpFJTJCUTVSUHJ4aGdibk5FJTJCVVBpTFVpa3o1SVI4eXYyJTJCZmNVWk1XciUyRkNFcWl1VFFnTFhLTzclMkZ1YXV5QWRjVFV4Z1Zoem0lMkZZbm9BNTVQTWRrMk51ZGlVcEZzeGJRUFZQenNHbnpLZkhmTVJ6NWhlRWVXdHRkOCUyQjN0cmRkalkxNVYlMkZaa3dieDNiUiUyQk9lV3R0bng3ekVVOVlYeExtJTJGWk56OE5vJTJCUGVZQyUyQlI4MUU5cXRxYTJ4b1pLVjBEYVh3NnVpVU5sUUxsUldsMHJ1UGhNSjM2dUZsV0Z6bVFzTlNNWktlRXR2d2NyV0pXTzFxTkloSjFKUGlENU4lMkZpUVhtRmFmV0lzbUhZVDRjRTNhUXR5ZEdIRUJSWm9mZiUyRm56VGJhY1BqNyUyQk1iaTBKVmxndnpLSDZ3JTJGVmo0MXpCeG83Tm16T2hCdXRIdiUyRkpXVEVyaiUyRjhwRHI3N0Z3JTNEJTNEJTNDJTJGZGlhZ3JhbSUzRSUzQyUyRm14ZmlsZSUzRfY8lE8AACAASURBVHhe7Z0JmE5lH8b/thl7qCxRyFKk+ETJUkgk6rNlRIyxJltEhCIJFQpFRaFsldJCSZpElkLWEJN9Jy0kY8l33Y/vHa+Z855z3vm/M2becz/X9V365pznOef8nvt9nvv8n+VkuHDhwgVhClsCo2evk10H/wjZ8xUrlEeebFEhZOWl9YJ2rxwop36PC9ltZs9bUopWeSFk5aX1gqg/XQ1Rfzp+1J+OH3OTgI9ABprF8BZD91cWS4+WNUL2kONmLpXxvWqGrLy0XtCWL6OkTPWnQnabW75/ScrUfz9k5aX1gqg/XQ1Rfzp+1J+OH3OTAM2iRzTAxlJX0eysdfyoPx0/6k/Hj/rT8WNuEqBZ9IgG2FjqKpqdtY4f9afjR/3p+FF/On7MTQI0ix7RABtLXUWzs9bxo/50/Kg/HT/qT8ePuUmAZtEjGmBjqatodtY6ftSfjh/1p+NH/en4MTcJ0Cx6RANsLHUVzc5ax4/60/Gj/nT8qD8dP+YmAZpFj2iAjaWuotlZ6/hRfzp+1J+OH/Wn48fcJECz6BENsLHUVTQ7ax0/6k/Hj/rT8aP+dPyYmwRoFj2iATaWuopmZ63jR/3p+FF/On7Un44fc5MAzaJHNMDGUlfR7Kx1/Kg/HT/qT8eP+tPxY24SoFn0iAbYWOoqmp21jh/1p+NH/en4UX86fsxNAjSLHtEAG0tdRbOz1vGj/nT8qD8dP+pPx4+5SYBm0SMaYGOpq2h21jp+1J+OH/Wn40f96fgxNwnQLHpEA2wsdRXNzlrHj/rT8aP+dPyoPx0/5iYBmkWPaICNpa6i2Vnr+FF/On7Un44f9afjx9wkQLPoEQ2wsdRVNDtrHT/qT8eP+tPxo/50/JibBGgWPaIBNpa6imZnreNH/en4UX86ftSfjh9zkwDNokc0wMZSV9HsrHX8qD8dP+pPx4/60/FjbhKgWfSIBthY6iqanbWOH/Wn40f96fhRfzp+zE0CNIse0QAbS11Fs7PW8aP+dPyoPx0/6k/Hj7lJgGbRIxpgY6mraHbWOn7Un44f9afjR/3p+DE3CdAsekQDbCx1Fc3OWseP+tPxo/50/Kg/HT/mJgGaRY9ogI2lrqLZWev4UX86ftSfjh/1p+PH3CRAs+gRDbCx1FU0O2sdP+pPx4/60/Gj/nT8mJsEaBY9ogE2lrqKZmet40f96fhRfzp+1J+OH3OTAM2iRzTAxlJX0eysdfyoPx0/6k/Hj/rT8WNuEqBZ9IgG2FjqKpqdtY4f9afjR/3p+FF/On7MTQI0ix7RABtLXUWzs9bxo/50/Kg/HT/qT8ePuUmAZtEjGmBjqatodtY6ftSfjh/1p+NH/en4MTcJ0Cx6RANsLHUVzc5ax4/60/Gj/nT8qD8dP+YmAZpFj2iAjaWuotlZ6/hRfzp+1J+OH/Wn48fcJECz6BENsLHUVTQ7ax0/6k/Hj/rT8aP+dPyYmwRoFj2iATaWuopmZ63jR/3p+FF/On7Un44fc5MAzaJHNMDGUlfR7Kx1/Kg/HT/qT8eP+tPxY24SoFn0iAbYWOoqmp21jh/1p+NH/en4UX86fsxNAjSLHtEAG0tdRbOz1vGj/nT8qD8dP+pPx4+5SYBm0SMaYGOpq2h21jp+1J+OH/Wn40f96fgxNwnQLHpEA2wsdRXNzlrHj/rT8aP+dPyoPx0/5iYBmkWPaICNpa6i2Vnr+FF/On7Un44f9afjx9wkQLPoEQ2wsdRVNDtrHT/qT8eP+tPxo/50/JibBGgWPaIBNpa6imZnreNH/en4UX86ftSfjh9zkwDNokc0EKixPH/+vHRo2UDadOgute5r4JrGuJlLZXyvmq7PT+8nsrPW1SD1p+NH/en4UX86fsxNAjSLHtGA3Zt1TNT9Uqx4KRk8crxrGjSLrlFZnrjl+5ekTP33dYWko9zUn66yaBZ1/Kg/HT/mJgGaRY9owK6x7PVYK1kSu0By584jRW4oJhkzZUqgMu3DhZaEaBZ1wqFZvMSP+nPWEs2iMyO7M9j+6fgxNwnQLHpEA3aNZeWbrpUsERGWJJZv3E+zKCJWnXWeIkmH4SMjI6RUieulZ5dH5OHGdQKqi2bxEhrqz7kRov6cGSXXLFJ/OrbM7S0CGS5cuHDBW4/sracNdoL378ePyecfzzJzGa0SI4sile5unQRN/JkzErdjr5w+fUZmTxkuUU3rWvKjWbT//VF/l/OxMovUn/s2nO2fe1Y8kwTsCNAshrk+7BrLM2fi5YPpk2Xb1p/l/LlzhsTOHdtk49rVsnH3SZrFAJHFQJI5fOS41KjXQXLnyiGrl7xHsygi1J+ugQlmGJr6S8qa+tPpj7lJwEeAZjHMtWDXWPbtFi2fzplxGYEcOXNJu8d6Sfc+z9IsBmkWAWzg0AkybuJsOXFwCc2ig1mk/pwbn2DMIvUXnFmk/pz1xzNIgGbRIxqwM4sVS+WTvoNGyH+btZI7yhSQxWt2yJjhg6R0mXLStlNPmsVkmMXm0f1l+Q8bZN/WL2gWHcwi9efcCAVrFqm/y5my/XPWGM8gATcEGFl0Qykdn2PXWMIgdunZX6I79ZQKN14l3/20U3bGbZPObRrJqq1HQmoWCxYsKDfffLMsXrxYWrRoIXPmzJFz/x/6DgVeTL3t16+fTJ48WfDfMTExMnr0aMmQIUNC8VOnTjV/r1atmnz//feuLmvVWY8al3SI+ezZc7J67Rb5+LNYadvqQZkycTDNooNZpP6cJUj9OTOyOyOttH/BPgXayPfff9+0ZW+88YZ06dJFli5dKtWrVw+2KNvzr7/+etm7d68p+5VXXpGPP/442eVPmjTJtK1ly5ZN0XtO9g0yo4oAzaIKX9rPbNdYPvd0D5kxZYL8sPmQtG/xgOzft0cwjzF7jpyybP3eFDOLP/zwg+zfv1+aNGkSMoCvvfaadO/eXTp16iQnTpyQWbNmyUcffZRwjb/++ktuuukmOXr0qFSpUkVlFjPkrhTwvu+pXlE+mDZS8l+bj2bRwSxSf87ytzKL1J8zN98ZaaX9c3/HF8/0N4s7duyQn376SWrVqiVXX311sEUFPP/gwYPy4IMPyurVqwVGb/fu3TJs2LBklf/7779LgQIFZObMmdKsWTNJqXtO1s0xU0gI0CyGBGPaLcSuscSiltnTJ0mT5m1k+y+bZdigJ+Sff05Jr/7PS+26DVVmccWKFdK+fXvZt2+f9OrVyzRGpUuXThJZHDdunPTs2VMQ9Xv22Wfl5MmTMmHCBNm6dauMGjXKRCM///xzQWTSP0rof3Nnz56VO+64Q3LkyGHekE+fPi3x8fFy1VVXJZz25JNPyoIFCyRLliySM2dOlVlcsGh5EjaZM2WW4sWukxLFi9iKgauhL+Gh/pzbDSuzSP05c3NjFlNSf+7v8OKZhw8fllatWgnazTp16pi27tNPP7WMLObLl0/uv/9+iYiIMNHHqlWrytixY6Vly5ayZ88eGTJkiDzxxBNSs2ZN+e6775Lcytdff22uERsbK++8845Mnz5devfuLZUqVTJlHDhwQDp06GDaUrSVaDv79OljyoGhRJRz2bJlkjdvXhkwYIA88MADggilL2GEp1ixYpdFQw8dOiQ9evQw10Qb3KBBAzPygzYaQYMff/zRGNWnnnpKMmfOLBMnTpT//ve/8s8//5hy5s2bZ9r0GjVqyNtvvy2FChUKFjHPVxKgWVQCTOvZ3Wwdcfy3o3Jg/14pWbqMZMyYUSIiIgM+lputc/79918pVaqUIJo3ZcoUWbRokSDyhyGUxMPQviGWunXrSrdu3UxjhcYEw8VoEPr27SuDBw82DSDOtUodO3aUbNmyyb333iu//vqreautX7++zJ492xhIGM/bbrtNvvrqK2NctWbRrs4/+jRWJk2dKwvmWn8Vh2YxKT3qL7Cigp2zSP1dzvJKtH/J6RPQ3uGFGS/KMIFdu3Y15shqGBovzn///bcxW8uXL5dp06ZJuXLl5KWXXjLtJdq/Y8eOmXYXxi9xQjSxc+fO8s0335hDkZGRpjy0oQsXLpQxY8aYF3SYVbShaMM3bNggt956q1SuXNkYUkz3+eSTT4zZ/Pbbb2XdunWmbX3xxReN6UV+/6Hzu+++W9avX2/acPQLeL5HH33UPDOiqHPnzjX5Hn74YWnbtq3phxD5RMCgf//+5lpot6Ojo03kEs8e7gl1HyhAkpxn15ZHs5gc6ukoj11juXtnnAzu11VWLI01jdLXK7bKM327yGM9+kvVu++1fEo3ZnH79u0mioiI4auvvmoavTx58sidd94Z0CxiDmPTpk2lXr16psHCUDIaLzScUVFRZngjUMK5uXPnNo3em2++KVu2bDGN1ogRI0xDgzKvueYamTFjhlSoUCFFzSLmM/YdNFYu/LXa8nZpFi9hof6cG5JgzSL1594sppT+nGs16RmFCxeWa6+91pguJAw548U6kFnEuRs3bjSRSEQWEfl7+eWXzYv10KFD5ZdffjFtsF1q3bq1MWZ4yS5atKgxmZkyZZI//vjDXBeRQ0T0YC4xref22283ZSJqifmNMH1Lliwxoz94Icd5H374oTFz/vMs8dJfsmRJefzxx+X11183t3TfffeZ0Z1Tp07JI488YiKkiD5iKBsv/zCjx48fNyNSiFTif40aNTJmFfeYXhO4btq0SVauXGlG3dDnwYQnNoUIuEADMOIwzqFI4A+zjX44OYlmMTnU0lEeO7MY1bC6xMefluatOsiIwX1k3uJ18vqYYbLl5/XyeezaZJtFXwPmM2so6LrrrrMchk48eRsNDYaLMRyNhLdJDLnATOLHZZXy589vjCKiiV988YVp6HLlyiWIVrZp08a8sWJeDhotDGOgTBhSnOOU2Fk7EbI/Tv1RfzoF6XJfCf0l547RfsEcoO1DwggLDFogs4hIIiKHMB4wGzCKMIyIxCG6CCMJ44UX9cQJRhPXQxkwbJhviDYSUUqk9957T5577jnT3sK0YJoP/oahZgxt+7frvrJ9ptLKLJ4/f97kGz58uDz99NMmC9pllPnbb78ZE+m/4BHPgcghIovZs2c389Cx8Ab3gX7krbfeMsPYKZlgahH9DGUCS0ROYf6wCAjcx48fb4w1ntFnGFHnMP04DyNqoUqIDmO6FqLIyTHcITOLqEiIAUJFBdslRJ4wbwIiYdIRQFTNzvTYNZa3Fs0pH8xfJmXKlZfyN14ln32zRo4eOSTtourLhl0nLG/MTWRx27ZtZjGJL7KIeSeILN51110BI4u+lX52ZtFuziLeePEmvGbNGjlz5oyJNKIsRBQxnydxKlGihMTFxZlGCT/UQAxpFu316cSP+qP+dC1c+tNfcp4XJggvvb7IIl5qYeQ0ZhHTeqzmLGKIGOYQL+Ro99B3Y2cKDG///PPPph295557zIs3hocxTAxjB6OBdh1zD9GmIrL47rvvmrmOGPZ2iizCfGE6ElLt2rUFCx1xD4gsBjKLuCeYTcxDRx+BPgVGZ/PmzcnB7CoP+ivMpcTIFubRI5KaHHPlfzE8A+ZiYjj9oYceSjCGq1atMpFdsPatdMdLArwRjHSoooq+e2nXrp0xi5heEOwQd8jMou/NAqtN0UHbJbw54A0Kb0VMySeAFcV4Y4MpwkRjK8Nj11k3rFVBChcpKo+26ypdY5rK86PekPmfvC8H9u2WeYvXJ9ss4g2qePHiZogBwwmYK4h/MVwSaM6iG7MIjVklvGXiDQzDL2hMjhw5Yt7K0cBh5bN/RBITt9EI4H4Qjsd/44eMSdxWDN0uMPDdF+aMTZ72iWeGoZ34UX/UX/JbOOecaVF/zned9AzM38M0GUS00ImjPYJp0ZhFBBJgOBIntImIMvkWt2CuIYaYcQ8Y+sWKa7zY40XwmWeeMXMbcT+INsLQoD3FIhO065hjiVEamBosmsEwJxapYHg68ZxFRDsxTQjtM9rpxx57zDxv4q3U/COLaNfhFXA91DWMGxIWxKRkQvT0+eefN88Fk4eIKK6NiGywCf0L2MJUYyGRv0lDX4koI0bBYMBxDAt78LLgW1SU+HqauYe4HowvDKNTUC/xdYMyi4FWJkFMmP/gS19++aVZUGC1ogp/Q8X7EsqE6YGwMHyJtyu8fWBI0SoBFKBC4HDHmM/m5YShCxgtCBpMEhseu856SewC6RrTzAxF+1JkZFZ5fcocubv2/ck2i8iIuRZYDY3GB/NNMJEZq/hQx/6NQzDD0Hb1jGgits6BScSPAG9QWF2X+M0s8ZxFaAnzGvEjwg81McNgty7x3aNX5iw68aP+Lp9vRP2FtrVOq/oL9ilhwNCXrV27Vho2bGjaMCxcQUQNC0z8jReibW6GoXFOoPTCCy+Y9hjlImKIOYN4wUdCNA1zEtEX46Ua7SjabUQpkQdRLwxZY7QIo4no+2E+ELjAXEmYHOT1v2fMR0T7jNXQMFyYq4d55XhOO7OIiCfmMGL1NdpozFnE0K3dswXL3up8RFsR9EL0FAlTl5DQV8A0YuGkmwS/AsMLk4zFRon7IzxTxYoVDUuwwfnoy3F9/OufQuF9UAamE8CjYfpCMCkosxhoZdLAgQPNmwKW4GMeBSoUptBqRRWGB2EE0bnD7AEU3mpgKhCKxpJ5zE+AgUxcIQAL4eCtBRNqAdM3RyKYhw6nc/GGBcOIKB4WgyQ2PE6rAQ8d3CffLpwvx44elvwFCknNOg9IgUKFAyJyMwyd3vjiTRr6Q0rMcN/3HaRM9acueySrTbkTP3OfHpdenvyPheMCFzt+AyavkR4tawSUBPUnJpJD/SW/1Uht/Y3tebcxLm7/hw7a7bnBnpeSZfvuJSWvkZbLxhB54g9HwOg2btzYBCXcJDCE+Uc5iMRaLWTB3FIEVhDNxBxTRFBxfuKhb5wD74MFnNgxBC8WePkMNsHUI1KMaQPBpKDMIpbmB1qZ5Asd+4ahA62owlsEwrF4Y8IwNExfmTJljGMHLIBAWNbKBAI8Filgs2WEyfHGg3kZwWxUilB6uCUMJ2A7A1+C4UHCJOKsZVvZdtY74rbKr9u2ms24/VODRtYR20ej28uPX00LN4RJngc/Rswteb59piRmMfHJR4/9Lnv2HZKyN90oGTNmkMjIi/ytUrcu0fL6jJ89w69QtW7UXzJqm/pLBjS/LD5+KaG/1V+/ZyJEbv8Hg+D23GDPS8mycS/puXztvWN+JobMkdCnwn8gKIbgGOZ1ukkwfwiQYZ4nFiElThhiR/AMU6YwVI3pAZjDCW+UeLga3gdzSOGP4KMQeYbRDJQw5xVzJBHk808wnQjSBbuAJyiziJBsoJVJic1ioBVVvrF7n1lEaBtOF5WBHzgSQtp4QLhf/4S3EAwvYkgTS/ERDg72gWkWLxEdPXyQvDlupKXWth06Z/n3cIwsYrjBN7fHTWTRByZux17p0muEfLN4lRk+2L5urnTuOVye7h0jdWrdYckvHCOLdvzsIovU30WJUH9uut3A51B/On7MnZQAtuyBwUO7jiFhDKHDJGIleTAJQ8tYwAKTh61/EicM4WORC7bSwVA0ppTB1+Bv/kPW/t4HI7fwMYGm6vmugQAchvoTRyAR8MuaNaspI5gUlFlEwYFWJvmbRdxIoBVVic2ib+UsIoRw7b6EuRH4n38CQLhrRNEwqRYPi/sJ9YqhYABe6XM1w9AVS+WTMreUl6jWHSVX7ktfO8Ez1brPemuCcDOLmPOEycvQkds5i746r1qnnZyOj5eO0Y2l94AxsnHl+zJ05CRZv2m7rF9uPUwRbmbRiZ/dNAjqT8z8a+ov+a2oEz/qL/lsvZwT0UAsEEIwCiYxuV+MwTxF7KGJ6XVY0IN+BgtnMI9x/vz5Zoga/+IrOEi+PYMT+xosDoKn8nkfBNmwVsTJ+8Ao/uc//7msKrEtEz6akaJmEQ8baGUSVk1hWBgTY7Gy55ZbbrFcUYUVq9gkGbu5Y9k99tDz7f2D/ZMwcRYuHPMfEW71Jd9+UgCEZfcY/x85cqRZSOHlpFngUrdaWRkyYnzADbituIabWXRaTWm3dU62/FVlxaKpUuG20pKzYA1Zt3ymHDx0TOo17ianDi+zlGW4mUUnfnadNfUnyVqN7xMW9efMj/rzcu+Y/GeHt8D2Qv6fMUxOaYgIYo4jIpKYLocdPTByitFR7LmIxULYU9E35OxbrexvFrE4CGbVN1SNxSmYEujzPjC0iK7DgLrZDgcr2jHFCqvWg0lBRRaxYivQyiTsD4UtTLBwBUYPq5cCraiC8UOIF/DwgXTMc8TKKUTJ4J4x/OxbIu97GB9E/4fzelRRu3UOtsmZMXWiPBrzuBQoePmiltvvrGapo3Azi077BNqZxdvuaiFFry8k3To3l8Yt+8hb4wbK7DkLZffegybKaJXCzSw68bPrrKk/Ue3zSf0586P+grEDPDclCOzdu9fsUgJfhBFU7PaCNRcwfNWqVTOLeYsUKWIuDU+DY1hAg72AkeB9MD0Kx3zJ531wDOYPxxFIc9oPEuYV3+3GinIYxmBSUGYxmIJT4lyAwaahf/75p9mzzykEmxL3kNbK1GzKXbrg5Uvz/Z/NS3MW7erUzix++fVyadKqj5w+fSahiKxZI+TjGaOk/n1VPWEWnX4Pdp019edET4T6c2Zkdwb1p+PH3KEh4FtZDoOITytiJxjsFoNoIXwMopi+hCl98DmIIPqSz/tg3QjMZOL5jIgSYn6ikyf67LPPzJA4hr6dzk385OnKLIam2rxVil1j+e7k8ZIli/XK3UeiO1uCCrfIopManL7gsm//EZm3YKkcPvKbFCp4jTSoV10KX5c/YLHhFll04kf9ORGyP0796fhRfzp+zB16AvjKGL6Qg4UqWNSCrQL9h4+xbQ4W++Jzh9hT0ynBeGIFNaKGTgnzJ7GROjb+DjbRLAZLLJ2d77TPYrCPQ7N4kdgff56Qrdt2SYniReTaay5to3D+/L8yZfpn0iE66co35KNZDFZxl59P/VF/wSiI7V8wtHhuahDAnqpY/4F9orGpOYaFEyd8zhCbZvsPPQe6t+bNm5tha6chaGzEjimA+FJQsFFFXJtmMTXUcQWvYdVYRj9cV7r3eVbGjxoa8M6mfbjQ8hg7a5HFS9fIQ1G95MTJU4Jh56kTh0hU07ryxcJl8tQzY+XnLTs887k/J2lTf06E7I9bRRapP/dMqT/3rHhm6hHwRQHtFqTgGBbC4BOKoUjYNxL7WrtZBGN1PZrFUNRCGi7DqrGsemtheWHMWzKwd6eAd758436aRUQCv4xKsil3ldptTVSxU0wTWRi7Ug4dPia3lCkhsd+tkmzZIqVrx+by8rBL20D5g2RkUYT6c99gUH/uWVmdyfZPx4+5ScBHgGYxzLXAYRhdBVt11jkKVJceXVrIiCHdZPXazVL5njYSEZFFOsU0loF92kvBAlcHvCjNoq4+GNkWof7ca4jtn3tWPJME7AjQLIa5PoJtLKe+NVZGDRsgm/b8zchigMhihtyVTOQQ33/eteeAFC/3kMx6Z7i0aFbXUU00i/aIqL/L+Vi9rFB/jj+zhBPY/rlnxTNJgGbRwxoItrF8e8JoeXFoP+HWORdF47aznjtzlDRqWNNRaTSL9oiov+SZRerPWlds/xybJJ5AAq4IMLLoClP6PYmNpa7uApnFG4oUNFvlxJ85I+s2bJPSJW+QvHlyJ1xsZexUywvTLNIsBqNI6i8YWknPZfun48fcJOAjQLMY5lpgY6mrYKvOGp/2c0onDy2lWRQR6s9JKfbHqT8dP+pPx4+5SYBm0SMasGosMScxUNqw9kdZuWwxh6H/D8hpU+RgZcTIopg5sdSfO+VQf+44BTqL7Z+OH3OTAM2iRzRg1VjafWbNh4VzFi+SYGet+6FQfzp+1J+OH/Wn48fcJECz6BENWDWWm9avcXz6cuVvtzyHW5c4orM9gZFFEerPvYZoFt2zsjqT7Z+OH3OTAM2iRzQQ7JwdJyw0i06E7I/TLOr4UX86ftSfjp/X9KejxdzhRIALXMKpNi2ehWZRV8GM7Oj4UX86ftSfjh/1p+PH3CTAyKJHNMDGUlfR7Kx1/Kg/HT/qT8eP+tPxY24SoFn0iAbYWOoqmp21jh/1p+NH/en4UX86fsxNAjSLHtEAG0tdRbOz1vGj/nT8qD8dP+pPx4+5SYBm0SMaYGOpq2h21jp+1J+OH/Wn40f96fgxNwnQLHpEA2wsdRXNzlrHj/rT8aP+dPyoPx0/5iYBmkWPaICNpa6i2Vnr+FF/On7Un44f9afjx9wkQLPoEQ2wsdRVNDtrHT/qT8eP+tPxo/50/JibBGgWPaIBNpa6imZnreNH/en4UX86ftSfjh9zkwDNokc0wMZSV9HsrHX8qD8dP+pPx4/60/FjbhKgWfSIBthY6iqanbWOH/Wn40f96fhRfzp+zE0CNIse0QAbS11Fs7PW8aP+dPyoPx0/6k/Hj7lJgGbRIxpgY6mraHbWOn7Un44f9afjR/3p+DE3CdAsekQDbCx1Fc3OWseP+tPxo/50/Kg/HT/mJgGaRY9ogI2lrqLZWev4UX86ftSfjh/1p+PH3CRAs+gRDbCx1FU0O2sdP+pPx4/60/Gj/nT8mJsEaBY9ogE2lrqKZmet40f96fhRfzp+1J+OH3OTAM2iRzTAxlJX0eysdfyoPx0/6k/Hj/rT8WNuEqBZ9IgG2FjqKpqdtY4f9afjR/3p+FF/On7MTQI0ix7RABtLXUWzs9bxo/50/Kg/HT/qT8ePuUmAZtEjGmBjqatodtY6ftSfjh/1p+NH/en4MTcJ0Cx6RANsLHUVzc5ax4/60/Gj/nT8qD8dP+YmAZpFj2iAjaWuotlZ6/hRfzp+1J+OH/Wn48fcJECz6BENsLHUVTQ7ax0/6k/Hj/rT8aP+dPyYmwRoFj2iATaWuopmZ63jR/3p+FF/On7Un44fc5MAzaJHNMDGUlfR7Kx1/Kg/HT/qT8eP+tPxbmjgxAAAIABJREFUY24SoFn0iAbYWOoqmp21jh/1p+NH/en4UX86fsxNAjSLHtEAG0tdRbOz1vGj/nT8qD8dP+pPx4+5SYBm0SMaYGOpq2h21jp+1J+OH/Wn40f96fgxNwnQLHpEA2wsdRXNzlrHj/rT8aP+dPyoPx0/5iYBmkWPaICNpa6i2Vnr+FF/On7Un44f9afjx9wkQLPoEQ2wsdRVNDtrHT/qT8eP+tPxo/50/JibBGgWPaIBNpa6imZnreNH/en4UX86ftSfjh9zkwDNokc0wMZSV9HsrHX8qD8dP+pPx4/60/FjbhKgWfSIBthY6iqanbWOH/Wn40f96fhRfzp+zE0CNIse0QAbS11Fs7PW8aP+dPyoPx0/6k/Hj7lJgGbRIxpgY6mraHbWOn7Un44f9afjR/3p+DE3CdAsekQDbCx1Fc3OWseP+tPxo/50/Kg/HT/mJgGaRY9ogI2lrqLZWev4UX86ftSfjh/1p+PH3CRAs+gRDbCx1FU0O2sdP+pPx4/60/Gj/nT8mJsEaBY9ogE2lrqKZmet40f96fhRfzp+1J+OH3OTAM2iRzQwevY62XXwj5A9bbFCeeTJFhVCVl5aL2j3yoFy6ve4kN1m9rwlpWiVF0JWXloviPrT1RD1p+NH/en4MTcJ0CxSAyRAAiRAAiRAAiRAAo4EMly4cOGC41k8gQRIgARIgARIgARIwJMEaBY9We18aBIgARIgARIgARJwR4Bm0R0nnkUCJEACJEACJEACniRAs+jJaudDkwAJkAAJkAAJkIA7AjSL7jjxLBIgARIgARIgARLwJAGaRU9WOx+aBEiABEiABEiABNwRoFl0x4lnkQAJkAAJkAAJkIAnCdAserLa+dAkQAIkQAIkQAIk4I4AzaI7TjyLBEiABEiABEiABDxJgGbRk9XOhyYBEiABEiABEiABdwRoFt1x4lkkQAIkQAIkQAIk4EkCNIuerHY+NAmQAAmQAAmQAAm4I0Cz6I4TzyIBEiABEiABEiABTxKgWfRktfOhSYAESIAESIAESMAdAZpFd5x4FgmQAAmQAAmQAAl4kgDNoiernQ9NAiRAAiRAAiRAAu4I0Cy648SzSIAESIAESIAESMCTBGgWPVntfGgSIAESIAESIAEScEeAZtEdJ55FAiRAAiRAAiRAAp4kQLPoyWrnQ5MACZAACZAACZCAOwI0i+448SwSIAESIAESIAES8CQBmkVPVjsfmgRIgARIgARIgATcEaBZdMeJZ5EACZAACZAACZCAJwnQLHqy2vnQJEACJEACJEACJOCOAM2iO048iwRIgARIgARIgAQ8SYBm0ZPVzocmARIgARIgARIgAXcEaBbdceJZJEACJEACJEACJOBJAjSLnqx2PjQJkAAJkAAJkAAJuCNAs+iOE88iARIgARIgARIgAU8SoFn0ZLXzoUmABEiABEiABEjAHQGaRXeceBYJkAAJkAAJkAAJeJIAzaInq50PTQIkQAIkQAIkQALuCNAsuuPEs0iABEiABEiABEjAkwRoFj1Z7XxoEiABEiABEiABEnBHgGbRHSeeRQIkQAIkQAIkQAKeJECz6Mlq50OTAAmQAAmQAAmQgDsCNIvuOPEsEiABEiABEiABEvAkAZpFT1Y7H5oESIAESIAESIAE3BGgWXTHKd2eNXr2Otl18I+Q3X+xQnnkyRYVQlZeWi9o98qBcur3uJDdZva8JaVolRdCVl5aL4j609UQ9afjR/3p+DE3CfgI0CyGuRa6v7JYerSsEbKnHDdzqYzvVTNk5aX1grZ8GSVlqj8Vstvc8v1LUqb++yErL60XRP3paoj60/Gj/nT8mJsEaBY9ogE2lrqKZmet40f96fhRfzp+1J+OH3OTAM2iRzTAxlJX0eysdfyoPx0/6k/Hj/rT8WNuEqBZ9IgG2FjqKpqdtY4f9afjR/3p+FF/On7MTQI0ix7RABtLXUWzs9bxo/50/Kg/HT/qT8ePuUmAZtEjGmBjqatodtY6ftSfjh/1p+NH/en4MTcJ0Cx6RANsLHUVzc5axy+Q/s6fPy8dWjaQNh26S637Gri+CFfju0ZleSJX41/EQv3pdMTc3iPArXPCvM5pFnUVTLOo42env5io+6VY8VIyeOR41xehWXSNimZRRKg/nV6YmwQYWfSIBmgWdRVNs6jjZ6e/Xo+1kiWxCyR37jxS5IZikjFTpoSLTftwoeWFaRZ19cHI4iV+1J9OS8ztLQKMLIZ5fdMs6iqYZlHHz05/lW+6VrJERFheYPnG/TSLImKlvzxFkm6KHxkZIaVKXC89uzwiDzeuE7DSaBYvoaH+dL9t5vYWAZrFMK9vmkVdBdMs6vgFq7/fjx+Tzz+eZeYyWiVGFkUq3d06CZr4M2ckbsdeOX36jMyeMlyimtblMLTDMLQVIOpP93tn7vAlQLMYvnVrnizYztoJBztrJ0L2xxnZucTnzJl4+WD6ZNm29Wc5f+6cObBzxzbZuHa1bNx9kmYxQGQxkMIOHzkuNep1kNy5csjqJe/RLDq0f+lRfwULFpSbb75ZFi9eLC1atJA5c+bIuf//dnQt08XcZ8+elQEDBsjMmTPlxIkTUrlyZRk7dqyUK1cuFMWzjHRMgGYxHVeem1unWXRDKfA5jCzq+Nnpr2+3aPl0zozLLpAjZy5p91gv6d7nWZrFIM0igA0cOkHGTZwtJw4uoVl0MIvpUX/+ZvGHH36Q/fv3S5MmTXQ/Ur/cAwcOlOHDh8sjjzwipUqVkldffVXy5csnO3fuDNk1WFD6JECzmD7rzfVd0yy6RmXduX4ZJWWqP6UrxC83I4uXYFQslU/6Dhoh/23WSu4oU0AWr9khY4YPktJlyknbTj1DahZTOiJz4cIF6devn0yePFnw3zExMTJ69GjJkCFDwnNMnTrV/L1atWry/fffu9JUsC8rzaP7y/IfNsi+rV/QLDqYxdTUn6vKtjhpxYoV0r59e9m3b5/06tVLJk2aJKVLl04SWRw3bpz07NlToLFnn31WTp48KRMmTJCtW7fKqFGjTDTy888/F/wO/DXpf0lEFaHPjBkzyrRp08whlImyDxw4IIUKFUruYzBfGBCgWQyDSrR7BJpFXQUH21k7XY1m8RIhGMQuPftLdKeeUuHGq+S7n3bKzrht0rlNI1m19UiKmcWUiMi89tpr0r17d+nUqZMZvps1a5Z89NFHCVGfv/76S2666SY5evSoVKlSRWUWR41LOsR89uw5Wb12i3z8Way0bfWgTJk4mGbRwSympv6c2gWr4//++6+J7kE7U6ZMkUWLFgl0Vr169SRm8Y033pAuXbpI3bp1pVu3btKyZUvJkiWLMX8weX379pXBgwfLkCFDBOdapc6dO19mJHHdu+66y1wfkcXMmTMn5zGYJ0wI0CyGSUUGegyaRV0F0yzq+Nnp77mne8iMKRPkh82HpH2LB2T/vj2CeWTZc+SUZev3qsxiakdk7rjjDsmRI4csXbpUTp8+LfHx8XLVVVclPMOTTz4pCxYsMB14zpw5VWYxQ+5KASvlnuoV5YNpIyX/tfloFh3MYkrqT/eruZh7+/btJoqI6B6Gg6GpPHnyyJ133hnQLGIOY9OmTaVevXqycOFC8+KSLVs2iYiIkKioKDMX0U3C8DaM559//mkikv/5z3/cZOM5YUyAZjGMKxePRrOoq2CaRR0/O/1hUcvs6ZOkSfM2sv2XzTJs0BPyzz+npFf/56V23YbJNoupHZHp2LGj6ZDvvfde+fXXX2XHjh1Sv359mT17tjGQGAq87bbb5KuvvjJDiVqzuGDR8iRsMmfKLMWLXSclihexrTBGti/hSSn96X4xl3Ljhadq1aoyYsQI6d+/vzlw3XXXWQ5D+yKLeFlB5LFZs2bm5QTD0UjQ3P33328WxGBI2yoVKXJJO61bt5b58+fL2rVrpWjRoqF6JJaTjgnQLKbjynNz6zSLbigFPodmUcfPjf6O/3ZUDuzfKyVLlzHzpSIiIgNe1M1q/NSOyCB6kzt3bomMjJQ333xTtmzZIi+++GJCJ48ozzXXXCMzZsyQChUqqM2iXY189GmsTJo6VxbMtf4qDs1iUnqh1p/uF3Mp97Zt28zUBV9k8Z9//jGRRQwNJ14NHYxZtJuz6Btq/uabbyR79uzmWkxJCWBeciCO/me7PS89MKZZTA+1pLhHN511MMW76ayDKS+tn0uzqKshO/3t3hkng/t1lRVLY82ikK9XbJVn+naRx3r0l6p332t5YTf6S+2ITP78+Y1RRDTxiy++MM+SK1cuM4zXpk0badWqlaxevdrMHatRo4YxixgixDlOKVj9YT5j30Fj5cJfqy2Lplm8hCWl9OdUp26PI0JevHhxOXXqlFk4hcg0/kW0UWMW582bZ3kLDRo0SDBAiEwWK1bMLI4J5wTGPkOHfzP5fUUK/x//Q4Ix9JlD5MG845EjR0rt2rVt8SDPJ598Iv/973/TPUaaxXRfhfYPQLOoq+BgO2unq4VbZ42omp3psdNfVMPqEh9/Wpq36iAjBveReYvXyetjhsmWn9fL57Frk20Wr0REBnPL8ubNK2vWrJEzZ86YSCM6XEQUsU9d4lSiRAmJi4szc8rs+AWrP6+ZRSd+V0J/Tm1AMMe//fZbsxr6+PHjZrU9VjtjKxu8EPnvsxhMZNHN9RHBxN6KblftuykzrZ0Dth9++KFMnDhRatasKTfccEPCKnDc6wMPPCBffvml3HLLLTJ+/HipVauWMY9YKIRzO3To4PhIe/bsMcP458+fN6Mm6TnRLKbn2nNx7zSLLiDZnBJsZ+10tXAyi5gEj0YWpggb+VqZHjv93Vo0p3wwf5mUKVdeyt94lXz2zRo5euSQtIuqLxt2nUi2WbwSERmsMh06dKgZMjxy5IhZDf3ee++ZCIT/HDF0MJjfiAgR5og58QtWf14yi2lVf05tAI+Licoh6o45mFcywfzBBOI3iVEB/6Fl3OPBgwcve9lDVBZbCWE+qFvzh5XpiFi+++67roauryQPu2vTLKbVmgnRfdEs6kAG21k7XS2czCKeFQ0tJtVjrhMWbyQ2jXb6a1irghQuUlQebddVusY0ledHvSHzP3lfDuzbLfMWr0+2WUTG1I7IIJqIrXNgEjHXq127djJs2LAkHUriOYtO/Kz0Z7XAxQcLcxYnT/vEM8PQTvyulP6c2gGvH8eL5d9//21+M2gzChQocEWQ+MwiIrfr169PMHPYV7JixYpmf0l/U9i2bVsTVcSLoduEl1eYRTwv2ob0mmgW02vNubxvmkWXoAKcRrNoz+/HH380hhHzqrA9B97M/U2jnf6WxC6QrjHNzFC0L0VGZpXXp8yRu2vfrzKLulpPvdxO/Kz0Z7d1ju/OvTJn0Ykf9Zd6Wg7mSlgI9sQTT5hhXXyusEePHoKvx1x99dXBFBOSc/G1Grxc+owh7ik6OtrMQcYxX4LpK1++vNnGCDsf+CfkwZA1RhYQScRIgy9KiWPXX3+9iUam588m0iyGRG5ptxCaRV3dhNosdusSLdeWelh3U2ks9zvvvCOYm+NLMI1IGGbKWraV9GhZI+AdHzq4T75dOF+OHT0s+QsUkpp1HpAChQoHPP/R6PbywF3F0xgB3e3Y8evd5K8kXxCy2pQ78R306dHa8qaov8uxBKu/fkPHS4UCfxkjgIiTb+GD77/D+W/+zxyK58S8XWxSj4QFYpjXBwMJ04g5k6mVXn75ZXnqqafMd7ExQoItg3zTSPyHpbHPJSKiP/30UxLTh22y1q1bJ48//rj06dNHypYtawyjL2G6CfZaffDBB1PrsUJ+HZrFkCNNWwXSLOrqg2bRmZ/GLO6I2yq/bttqNuP2Tw0aRVlemGYxKZajx36XPfsOSdmbbpSMGTNIZORFs26VaBYvpxKs/trEdJa6lQubiJhvJa3vX/7t4upht1wwGgGD6J9gGhs3bmzMWmolbBNUp04dsz8q5hFjg318LSdxlPOXX34xn008duzYZcfwzM2bNzcLYhCR/PTTT435xf/3JSxSwrB2165dU+uxQn4dmsWQI01bBdIs6uoj1GYx3OYsaoYBRw8fJG+OG2lZQdsOnbP8u5utc3Q1nrq5nfjZ6S9ux17p0muEfLN4lemkt6+bK517Dpene8dInVp3WD4I9XcJC/WXulr3vxqGnmHG8ClBJGxejygeNh/HNJbUnNuHfVERCVy+fLkZSsbeltjuKnHCbxVfz8F+l1mzZr3sMFZIYx4jzCY+sYioov9cR6xkR57nnnvuykFXXplmUQkwrWenWdTVEM2iPT/NAoOKpfJJmVvKS1TrjpIr96VP4+GKte5r4Amz6MTPTn9V67ST0/Hx0jG6sfQeMEY2rnxfho6cJOs3bZf1y60jM+FmFp342bV/1J+ubdTkxkpjmCsYKiz+ePrpp41JREQutRM+aYhh7+nTp5tPGyKqabXhNhaoYI9UTLnBHERfwheaMPUGEVUMYWMoPfFWOZjHiO980yymdu3yeq4J0Cy6RmUdifkyKsmcMU2J4dRZa7cuqVutrAwZMT7gBtxWnMMpsuiGn51ZzJa/qqxYNFUq3FZachasIeuWz5SDh45Jvcbd5NThZWEfWXTDz67987r+NO2YJq9v7h8ibc8884wxib4vx2jKTW5emLxrr71W7rnnHhk0aJAZLrZKOA+RRSzO8Z2DiD7+hoUtiEb6Vj77m0Wcg834cbxLly7Jvc0rno+RxSteBSl7AzSLOr6MLNrz02yKjG1yZkydKI/GPC4FCl6+qOX2O6t5IrLoxM9Of7fd1UKKXl9IunVuLo1b9pG3xg2U2XMWyu69B02U0SqF08sKns+Jn137R/3p2kZN7tdee026deumKSJkeWHw8FnDqKgo6d27d8BycR62znnooYfM3rK+hC/qYJN0X8JiF5TlS77V0PjqTsmSJUN236ldEM1iahNP5evRLOqA0yzq+Nnpr3TBzAEL98qcRSe6dvr78uvl0qRVHzl9+kxCMVmzRsjHM0ZJ/fuqesIsOvGj/pwI8TjMHLbOwYplp422MXyOTfZhdn0JJhKruteuXWs24U+8khtD09igf+bMmY7lp+XaoFlMy7UTgnujWdRBpFnU8bPT37uTx0uWLNYrdx+J7mx54XAahnZD1kl/+/YfkXkLlsrhI79JoYLXSIN61aXwdfkDFh1ukUUnhtSfEyEeD4YAhpcrV64s3333natvu8OIYq4iNujH5xnTc6JZTM+15+LeaRZdQLI5xamzDrZ0dtbBErv8fJrFizz++POEbN22S0oULyLXXpM3AdL58//KlOmfSYfoRowsigjbP93vjbmTEti+fbtZDOO/8XYgTvg0IIaf58yZk66jing+msUw/zWwsdRVMM2ijp+V/qIfrivd+zwr40cF/mTWtA8XMrIoIlb6W7x0jTwU1UtOnDwlGHaeOnGIRDWtK18sXCZPPTNWft6ywzOf+3NSJ/XnRIjHk0MA8xKxCtp/4+3E5WCT7iZNmkhcXFy6N4o0i8lRSTrLQ7OoqzCaRR0/K/1VvbWwvDDmLRnYu1PAwpdv3E+zGMAsVqnd1kQVO8U0kYWxK+XQ4WNyS5kSEvvdKsmWLVK6dmwuLw/rychigMgi9af7TTO3Nwkwshjm9U6zqKtgmkUdP+pPx89KfzkKVJceXVrIiCHdZPXazVL5njYSEZFFOsU0loF92kvBAoG/r8tpELr68No0CB0t5g4nAjSL4VSbFs/CzlpXwTSLOn7B6m/qW2Nl1LABsmnP34wsBogsZshdyUQO8f3nXXsOSPFyD8msd4ZLi2Z1HSuLZtEeEfXnKCGe4FECNIthXvHBdtZOOLz2Zk2z6KQI++PB6u/tCaPlxaH9hFvnXORqpT8rszh35ihp1LCmY2XRLNojov4cJcQTPEqAZjHMKz7YztoJB82iEyH74+ys2VkHo6BAZvGGIgXNVjnxZ87Iug3bpHTJGyRvntwJRa+MnWp5GeqP+gtGfzyXBHwEaBbDXAs0i7oKZmRRxy9Y/TGyczlvK/3h035O6eShpTSLydg6h/pzUhaPe5UAzWKY13ywnbUTDkYWnQgxsuhPwEp/mJMYKG1Y+6OsXLaYw9D/B8SXFd3vjfrT8WNuEmBk0SMaoFnUVTQ7ax0/K/3ZfebPdzXOWbxIgvqj/nQEmJsEQkOAkcXQcEyzpQQyi/hsUYeWDaRNh+5S674Gru+fkUXXqCxP5JwxkU3r1zhCLFf+dstzqD9HdLYnUH/Un05BzO1VAjSLYV7zdpHFmKj7pVjxUjJ45HjXFNhZu0ZFs5iMOWNOdKk/J0L2x2kWdfy8pj8dLeYOJwI0i+FUmxbPYmcWez3WSpbELpDcufNIkRuKScZMmRJK4OfWLqLgMKDuB8JpEDp+1J+OH/Wn48fcJOAjQLMY5lqwaywr33StZImIsCTAz60FNot5iiTdzy4yMkJKlbheenZ5RB5uXCegqhjZ0f3gvBbZoVnU6YVmUcePuUmAZtEjGgi2sfz9+DH5/ONZZi6jVWJnLVLp7tZJ0GC/u7gde+X06TMye8pwiWpq/TUNmkXdD4/60/Gj/nT8vKY/HS3mDicCjCyGU21aPIudWTxzJl4+mD5Ztm39Wc6fO2dy79yxTTauXS0bd5+kWQxyGPrwkeNSo14HyZ0rh6xe8p4lP3bWuh+c1zprRhZ1egn2Zdnpal7TnxMPHvcOAZrFMK9ru8ayb7do+XTOjMsI5MiZS9o91ku693mWZjFIswhgA4dOkHETZ8uJg0toFrnARd260CzqENIs6vgxNwn4CNAshrkW7BrLiqXySd9BI+S/zVrJHWUKyOI1O2TM8EFSukw5adupJ81iMsxi8+j+svyHDbJv6xc0izSL6taFZlGHkGZRx4+5SYBm0SMasGssYRC79Owv0Z16SoUbr5LvftopO+O2Sec2jWTV1iMhNYsFCxaUm2++WRYvXiwtWrSQOXPmyLn/D32HoiouXLgg/fr1k8mTJwv+OyYmRkaPHi0ZMmRIKH7q1Knm79WqVZPvv//e1WWtOutR45IOMZ89e05Wr90iH38WK21bPShTJg6mWRSR0bPXya6Df7hi7eakYoXyyJMtKrg5NSzO2b1yoJz6PS5kz5I9b0kpWuWFkJWX1gui/tJ6DfH+0gsBRhbTS00l8z7tzOJzT/eQGVMmyA+bD0n7Fg/I/n17BPMYs+fIKcvW700xs/jDDz/I/v37pUmTJsl8qqTZXnvtNenevbt06tRJTpw4IbNmzZKPPvoo4Rp//fWX3HTTTXL06FGpUqWKyixmyF0p4H3fU72ifDBtpOS/Nh/NYshqlwWRAAmQAAlcSQI0i1eSfipc284sYlHL7OmTpEnzNrL9l80ybNAT8s8/p6RX/+eldt2GKrO4YsUKad++vezbt0969eolkyZNktKlSyeJLI4bN0569uwpiPo9++yzcvLkSZkwYYJs3bpVRo0aZaKRn3/+uSAy6R8l9L+5s2fPyh133CE5cuSQpUuXyunTpyU+Pl6uuuqqhNOefPJJWbBggWTJkkVy5sypMosLFi1PwiZzpsxSvNh1UqJ4Edta9doCl1SQOC9BAiRAAiSQwgRoFlMY8JUu3s2cneO/HZUD+/dKydJlJGPGjBIRERnwtt2sBvz333+lVKlSgmjelClTZNGiRYLIX/Xq1ZOYxTfeeEO6dOkidevWlW7duknLli2NocNwcaFChaRv374yePBgGTJkiOBcq9SxY0fJli2b3HvvvfLrr7/Kjh07pH79+jJ79mxjIGE8b7vtNvnqq6+McdWaRbs6/ejTWJk0da4smGv9VRyaxSv9i+D1SYAESIAEgiVAsxgssXR2vp1Z3L0zTgb36yorlsaaeX5fr9gqz/TtIo/16C9V77432ZHF7du3mygiIoavvvqqifLlyZNH7rzzzoBmEXMYmzZtKvXq1ZOFCxeaoWQYwIiICImKipKZM2cGJI9zc+fOLZGRkfLmm2/Kli1b5MUXX5QRI0ZI//79TZnXXHONzJgxQypUqJCiZhHzGfsOGisX/lpteb80i+nsB8TbJQESIAESEJrFMBeBnVmMalhd4uNPS/NWHWTE4D4yb/E6eX3MMNny83r5PHZtss0ihqCrVq2aYNZQ0HXXXWc5DO2LLGL4GJHHZs2ameFiDEcjIQp4//33mwUxGNK2Svnz5zdGEdHEL774whjfXLlymWhlmzZtpFWrVrJ69WoTqaxRo4YpE4YU5zilYFej0iw6EeVxEiABEiCB9EYgZGYR88aGDx9uhg2zZ89uywGRp6+//loef/zx9MYrzd0vomp2psfOLN5aNKd8MH+ZlClXXsrfeJV89s0aOXrkkLSLqi8bdp1Itlnctm2bWUziiyz+888/JrJ41113BYwsujGLdnMWEcnMmzevrFmzRs6cOWMijTCeiCiOHTs2ybOUKFFC4uLizIppLIoJxJBmMc1JnjdEAiRAAiSQygRCZhbnzZsnDz74oFltig7aLvXp08dEjzZt2pTKjxtel8OK4po1axpTNGDAAEvDY2cWG9aqIIWLFJVH23WVrjFN5flRb8j8T96XA/t2y7zF65NtFjFnsXjx4nLq1CmzlQ3mCuJfRBsTb50TTGQRGrNKDRo0MHMahw4dagzqkSNHzGro9957z6x89o9IdujQwQxv434wLI7/Pn/+vGABjBVDK7NotcDFd1+Yszh52icchg6vnxqfhgRIgAQ8TSAos4gIERYjoNPGPDQM6b399tvyzTffSOvWl76X++WXX5oFBeiYETHCsB86Y5hE/A15fAllwvSgXAxfYkgRiyEwpGiVMMSISBEWKmABA+azeTnVqlXLMM6cObNhktjw2JnFJbELpGtMMzMU7UuRkVnl9Slz5O7a9yfbLCLjt99+a1ZDHz9+3Ox/iNXO+fLlM3Xsv89iMGbRrp4RTcTWOTCJiGy3a9dOhg0bZhbs+KeeIpf3AAAbU0lEQVTEcxahJcxrhMFF5DIxQyuzaLd1ju9anLPo5V8ln50ESIAEwotAUGYRW5mgY/3kk0+MAYyOjjZRrYEDB5qIzvTp083K18qVKxtTiC1PPv30U2PqsCp2w4YNZngQRhCdO/5esWJFuf32242pwLy0iRMnyscff2wMJFay+id06Fj5umTJErPCFQbpww8/NPfg1fTjjz8KDCOieFgMktjwOK2GPnRwn3y7cL4cO3pY8hcoJDXrPCAFChUOiNPNauj0VhdXX3210R9SYob7vu8gZao/ddkjWW3KnfiZ+/S49PLkf4wLXNKbOni/JEACJEACQZnFl156yUSJ8L9GjRoZU5gpUyZDEVFDzP/yDUP/8ccfZqEB5pH5hqgR9UFUqVKlSmYvPAxDw/SVKVPGmFCsXsVK1rJly1qaQJhFLFLAZsuY/4bIEaJJiFi5Tc8995zbU9PNee+8847s2bMn4X5heJCwuCNr2VbSo2WNgM+yI26r/Lptq9mM2z81aGQdsX00ur38+NW0dMMmuTeK7Xseeugheb59piRmMXGZR4/9Lnv2HZKyN90oGTNmkMjIi/ytEs1icmuE+UiABEiABK4UgaDMIvbNw2IARP6woAUrXN966y3BnLHEZhHzxWDMMF8MJg/n42+PPvroZWbxu+++M/PuYHDQQSP9/fffZkNmDF37J5hPDC9iSLNkyZLyxBNPSNeuXYNiR7N4Cdfo4YPkzXEjLfltO3TO8u/hGFnEHNvffvvNPK+byKIPTNyOvdKl1wj5ZvEq82K0fd1c6dxzuDzdO0bq1LqDZjGoXyZPJgESIAESSKsEgjKLeAgsBkBUEPPkMPSMyOLmzZsvM4tZs2Y1EcV77rnHbGUyd+5cE1G0Mou+lbPYkBnl+RKihYkjhqtWrTKRRUTRsPUJjB/uJ/G8tLQKOyXuSzMMXbFUPilzS3mJat1RcuW+9LUT3Get+xp4wixizuLTTz9tdOR2zqIPTNU67eR0fLx0jG4svQeMkY0r35ehIyfJ+k3bZf3yWTSLKSF4lkkCJEACJJDqBIIyi4899phZxYwFKlhFisgeEgzLM888Y4aFX3nlFbO/3S233GKGijE0jWNYBINIIVasYpPk9evXy7vvvmv20MOn2hBNRJRy+fLlxlxi/iOGo30JQ9a33nqrKad27dpmhe3IkSPNQgovJ80Cl7rVysqQEeMDbsBtxTXcIovJWQ3t45Itf1VZsWiqVLittOQsWEPWLZ8pBw8dk3qNu8mpw8toFr38w+SzkwAJkEAYEQjKLGJIGQtMli1bZoaWMWdx/PjxUq5cOVm3bp0ZjsbCFRg9nAPjiNXN2KYERhIrYTHsDOPXo0cPwcKCn376ycxzxGpomE5EJGEqfUbUxxrX882P9P3N61FF7dY52CZnxtSJ8mjM41Kg4OWLWm6/s5onIouafRZvu6uFFL2+kHTr3Fwat+wjb40bKLPnLJTdew+aKKNV4pzFMGo9+SgkQAIk4BECQZnFK80EhvHnn3+WP//80+zZ5+XhZ19daDblLl0wc8Aq9dKcRTtd223K/eXXy6VJqz5y+vSZhCKyZo2Qj2eMkvr3VaVZvNINBq9PAiRAAiQQEgLpyiyG5Ik9Vojd1jnvTh4vWbJYr9x9JLqzJyKLTnJw+oLLvv1HZN6CpXL4yG9SqOA10qBedSl8Xf6AxTKy6EScx0mABEiABNIaAZrFtFYjIb4fp30Wg71cuM1ZdHr+QGbxjz9PyNZtu6RE8SJy7TV5E4o5f/5fmTL9M+kQ3YiRRSe4PE4CJEACJJAuCNAspotqSv5NWpnF6IfrSvc+z8r4UUMDFjztw4WMLIqIlVlcvHSNPBTVS06cPCUYdp46cYhENa0rXyxcJk89M1Z+3rKDn/tLvmSZkwRIgARIII0RoFlMYxUS6tuxMotVby0sL4x5Swb27hTwcss37qdZDGAWq9Rua6KKnWKayMLYlXLo8DG5pUwJif1ulWTLFildOzaXl4dd2gbKHySHoUOtcJZHAiRAAiSQ0gRoFlOa8BUun8PQugqwiizmKFBdenRpISOGdJPVazdL5XvaSEREFukU01gG9mkvBQtcHfCiXjOLo2evk10H/9BVgl/uYoXyyJMtKoSsvLRe0O6VA+XU73Ehu83seUtK0SovhKy8tF4Q9ZfWa4j3l14I0Cyml5pK5n0GaxanvjVWRg0bIJv2/M3IYoDIYobclUzkEN9/3rXngBQv95DMeme4tGhW17GWvGYWg9WfE0DOmXUiZH+c+tPx85r+dLSYO5wI0CyGU21aPEuwnfXbE0bLi0P7CbfOuQjTKrJoZRbnzhwljRrWdFQTO2tHRLYneK2zdlqNHyxN6i9YYpef7zX96WgxdzgRoFkMp9qkWQx5bQYyizcUKWi2yok/c0bWbdgmpUveIHnz5E64/srYqZb3ws5aV0Ve66xpFnV6CfZl2elqXtOfEw8e9w4BmsUwr+tgG0tGFi8XhFVnjU/7OaWTh5bSLIpIsPpz4uq1zppm0UkR9sepPx0/5iYBHwGaxTDXglVjiTmJgdKGtT/KymWLOQz9f0DsrHU/EHbWOn7Un44f9afjx9wkQLPoEQ1YNZZ2n/nzYeGcxYsk2FnrfijsrHX8qD8dP+pPx4+5SYBm0SMasGosN61f4/j05crfbnkOhwEd0dmewDmLF/GcP39eOrRsIG06dJda9zVwDZX6c43K8kTqj/rTKYi5vUqAw9BhXvN8s9ZVMCM7On52+ouJul+KFS8lg0eOd30RmkXXqGgWHebMUn86LTG3twjQLIZ5fdMs6iqYZlHHz05/vR5rJUtiF0ju3HmkyA3FJGOmTAkX4+cmL6Kg/qg/HQHmJoHQEKBZDA3HNFsKzaKuathZ6/jZ6a/yTddKlogIywvwc5OBzWKeIkn384yMjJBSJa6Xnl0ekYcb1wlYaRyGvoSG+tP9tpnbWwRoFsO8vmkWdRVMs6jjF6z+fj9+TD7/eJaZy2iVOAwtUunu1knQYL/PuB175fTpMzJ7ynCJamr9NSGaRXs9p3X9FSxYUG6++WZZvHixtGjRQubMmSPnzp3T/Uj9cp89e1YGDBggM2fOlBMnTkjlypVl7NixUq5cuZBdgwWlTwI0i+mz3lzfdbCdtVPB7KydCNkfZ2d9ic+ZM/HywfTJsm3rz3L+/x3ezh3bZOPa1bJx90maxSCHoQ8fOS416nWQ3LlyyOol71nyo/7St/78zeIPP/wg+/fvlyZNmugaJb/cAwcOlOHDh8sjjzwipUqVkldffVXy5csnO3fuDNk1WFD6JECzmD7rzfVd0yy6RmXduX4ZJWWqP6UrxC83O+tLMPp2i5ZP58y4jG2OnLmk3WO9pHufZ0NqFlM6InPhwgXp16+fTJ48WfDfMTExMnr0aMmQIUPCc0ydOtX8vVq1avL999+70lSwke2BQyfIuImz5cTBJTSLDgtcUlN/rirb4qQVK1ZI+/btZd++fdKrVy+ZNGmSlC5dOklkcdy4cdKzZ0+Bxp599lk5efKkTJgwQbZu3SqjRo0y0cjPP/9c8Dvw16T/JRFVhD4zZswo06ZNM4dQJso+cOCAFCpUKLmPwXxhQIBmMQwq0e4RaBZ1FRxsZ+10NZrFS4QqlsonfQeNkP82ayV3lCkgi9fskDHDB0npMuWkbaeeKWYWUyIi89prr0n37t2lU6dOZvhu1qxZ8tFHHyVEff766y+56aab5OjRo1KlSpUUM4vNo/vL8h82yL6tX9AsOpjF1NSfU7tgdfzff/810T1oZ8qUKbJo0SKBzqpXr57ELL7xxhvSpUsXqVu3rnTr1k1atmwpWbJkMeYPJq9v374yePBgGTJkiOBcq9S5c+fLjCSue9ddd5nrI7KYOXPm5DwG84QJAZrFMKnIQI9Bs6irYJpFHT87/cEgdunZX6I79ZQKN14l3/20U3bGbZPObRrJqq1HVGYxtSMyd9xxh+TIkUOWLl0qp0+flvj4eLnqqqsSnuHJJ5+UBQsWmA48Z86cKrM4alzSIeazZ8/J6rVb5OPPYqVtqwdlysTBNIsOZjEl9af71VzMvX37dhNFRHQPw8HQVJ48eeTOO+8MaBYxh7Fp06ZSr149WbhwoXlxyZYtm0REREhUVJSZi+gmYXgbxvPPP/80Ecn//Oc/brLxnDAmQLMYxpWLR6NZ1FUwzaKOn53+nnu6h8yYMkF+2HxI2rd4QPbv2yOYx5g9R05Ztn5vss1iakdkOnbsaDrke++9V3799VfZsWOH1K9fX2bPnm0MJIYCb7vtNvnqq6/MUKLWLGbIXSlgpdxTvaJ8MG2k5L82H82iQ/uXUvrT/WIu5cYLT9WqVWXEiBHSv39/c+C6666zHIb2RRbxsoLIY7NmzczLCYajkaC5+++/3yyIwZC2VSpSpEjCn1u3bi3z58+XtWvXStGiRUP1SCwnHROgWUzHlefm1mkW3VAKfA7Noo6fnf6wqGX29EnSpHkb2f7LZhk26An5559T0qv/81K7bsNkm8XUjsggepM7d26JjIyUN998U7Zs2SIvvvhiQiePKM8111wjM2bMkAoVKqjN4oJFy5OwyZwpsxQvdp2UKH6pw7cCyGkQl6iklP50v5hLubdt22amLvgii//884+JLGJoOPFq6GDMot2cRd9Q8zfffCPZs2c312IiARCgWQxzHdAs6iqYZlHHz43+jv92VA7s3yslS5cxk+sjIiIDXtTNavzUjsjkz5/fGEVEE7/44guzwCVXrlxmGK9NmzbSqlUrWb16tZk7VqNGDWMWMUSIc5xSsPr76NNYmTR1riyYa/1VHJrFpMRDrT+nOnV7HBHy4sWLy6lTp8zCKUSm8S+ijRqzOG/ePMtbaNCgQcKcRUQmixUrZhbHhHvCy+XevXuldu3aSR71/fffl7Zt25o6CGSyg+UDw79q1SozHzU9JZrF9FRbybhXN511MMW66ayDKS+tnxtsZ+30POysLxHavTNOBvfrKiuWxhqD9fWKrfJM3y7yWI/+UvXuey1RutHflYjIYG5Z3rx5Zc2aNXLmzBkTaUSHi4gi9qlLnEqUKCFxcXFOcgn6Cy6Yz9h30Fi58Ndqy7Kpv5TXn2OlBnHCt99+a1ZDHz9+3Ky2x2pnbGWDFyL/fRaDiSy6uTwMDfZWdLtq302ZafEctDtY2INpJIl/pzDrmTJlkuXLl4c0wvr222/Lp59+Kp988ol5OU4viWYxvdRUMu+TZjGZ4P6fjWbRnh+GYO0iZHb6i2pYXeLjT0vzVh1kxOA+Mm/xOnl9zDDZ8vN6+Tx2bbLN4pWIyGCV6dChQ82Q4ZEjR8xq6Pfee8+sfPafI9ahQwfTMSFChIUKTvyC1Z/XzKITvyuhP12Lw9ypSQDR2pEjR5qoP6aJ+EcPsfL8l19+kfHj3X+73u29Y34opqpg1CG9JJrF9FJTybxPmsVkgqNZdASHFZM1a9Y0ETR89cHKNNrp79aiOeWD+cukTLnyUv7Gq+Szb9bI0SOHpF1Ufdmw60SyzSIypnZEBtFEbJ0Dk4i5Xu3atZNhw4YliRz4z1l0w49mMbAM3fC7Uvpz/PF4/AQYNEzRwIKdK5XwUon5xNhf0tdm+Myi7xiigDfccEPIb3HDhg1Svnx58/UdRC/TQ6JZTA+1pLhHmkUFvCC/oOHmSuE2DFirVi2zXQwmxmOlb2LTaKe/hrUqSOEiReXRdl2la0xTeX7UGzL/k/flwL7dMm/xepVZdFMXaeEcJ35WZtFqgYvvWTBncfK0TzwzDO3Ej/pLCypPeg94sfz777/NCxbajAIFCqT6jWIvSqwcX7JkiZlTjLmLPrO4a9cueeihh2T9+vVJ5irCSCKivWnTJjM8nZyhZAx/X3311WbaCualpodEs5geaklxjzSLCng0i47wfvzxR0GHjQng2MsNja2/abTT35LYBdI1ppkZivalyMis8vqUOXJ37fs9YRad+FmZRbutc3zQvDJn0Ykf9ef4E74iJ2DXgCeeeMLMVUZ0rUePHoJPDcJApUbavHmz3HLLLebamD6CIWGYV1+U7/XXX5evv/7azCv0T7hfbGXku9fffvvNPEOwCXkqVapkvvKE0Zn0kGgW00MtKe6RZlEBLwXMYrcu0XJtqYd1N5XGcr/zzjuyZ8+ehLuCaUTCMFPWsq2kR8saAe/40MF98u3C+XLs6GHJX6CQ1KzzgBQoVDjg+Y9Gt5cH7kofb+Juq8mOX+8mfyX53KTVptyJr9WnR2vLy1N/l2MJVn/9ho6XCgX+Mi9FiCjhX///Due/hfo5scgLXzRCwm4C58+fNwYSRgwLbFIylSxZ0swXfO6558yXbWAYjx07lmBW8Td84hCfV/RPmPrwwAMPyLp168wcR3wKEV+Eclopja/gYNGbf3r44YelYcOGpp10yp+SLNyWTbPollQ6PY9mUVdxwc4Zc7oaO+vLCe2I2yq/bttqNuP2Tw0aRVmipFlMiuXosd9lz75DUvamGyVjxgwSGXnRrFsl6k+nvzYxnaVu5cImmoThSP9/+bcLQXHBaAQMon+CaWzcuLGZ+5tSyTdf8JlnnjHbA+3evduYRQwrI9qIBLOIeciIIvonbFJesWJFefzxx+W+++6TW2+9VWB67RJWreMrOLgOdkfwpaeeesqYYgzDp4dEs5geaklxjzSLCngpEFkMtzmLmmHA0cMHyZvjRlpW0LZD5yz/7mbrHF2Np25uJ352LytxO/ZKl14j5JvFq0wnvX3dXOncc7g83TtG6tS6w/JBqL9LWKi/1NW6/9Uw/IshZ0TckPClI0TX8KUaTGPBIrGUSPidYOshmDysSMZ2V/j/WIyGjch9ey2OGTPGrITGcLl/Qv7mzZubfVJx7x9++KFZ4GeXXnnlFfnss8/Mojv/hG95Y49WRDfTQ6JZTA+1pLhHmkUFPJpFR3iaBQYVS+WTMreUl6jWHSVX7kvfUcZFa93XwBNm0YmfnVmsWqednI6Pl47RjaX3gDGyceX7MnTkJFm/abusX24dmQk3s+jEz679o/4cf94pdgJWQyOah2F7zBN8+umnjUlEZDElExaUYOh4ypQpCQtTYmNjzac6/c0i5ipOnDjRbITuS4gOwkQuWrRI/vjjD+nTp49ZKe0ze9ho+/bbb3e94AULaPDNbZrFlKxxlu2aAM2ia1TWkZgvo5LMGdOUGE6dtXbrkrrVysqQEeMDbsBtxTmcIotu+NmZxWz5q8qKRVOlwm2lJWfBGrJu+Uw5eOiY1GvcTU4dXmat5+9fkjL139dIOM3kdcPPrv3zuv6uVEXGx8ebbbayZs0qGAqGSfR9ZjAl7wlfrkHkEkPciCwiYSoBIn516tQxJhAvHzCwMH74csvGjRsTzB/2XVy5cqVMnz5dsLAF9419U++++25zHr7/DgP50ksvOc5B9C1wgVHEvMX0kBhZTA+1pLhHmkUFPEYWHeFpNkXGNjkzpk6UR2MelwIFL1/Ucvud1TwRWXTiZ2cWb7urhRS9vpB069xcGrfsI2+NGyiz5yyU3XsPmiijVQqnlxU8nxM/u/aP+nP8eafYCTBe3bp1S7HyExeMTfP9I3i+FcyJ/966dWt599135fDhw2ZLG6yQ9t97sWDBggmLcrD4BWYRCWYRm3fjbzCgTgtWcA72mMSqbAyDp4dEs5geaklxjzSLCng0izp4ImKnv9IFMwcs3ytzFp0A25nFL79eLk1a9ZHTp88kFJM1a4R8PGOU1L+vqifMohM/6s+JkHeOwyBamTifcfQ/hr/BzM2fP98saPEl3yIm38pwf3pYrAPz+fzzzztCxQ4ICxYskNmzZ7setnYsNIVPoFlMYcBXuniaRV0NhHo1dLhFdpzo2unv3cnjJUsW65W7j0R3tiw6nIahndjhuJP+9u0/IvMWLJXDR36TQgWvkQb1qkvh6/IHLJr6u4SG+nOjQO+egw258cUlt/soYr/EkydPmg2+7RLKu/HGG81nBqOirHd9SIvUaRbTYq2E8J5oFnUwnTrrYEtnZx0sscvPp1m8yOOPP0/I1m27pETxInLtNXkTIJ0//69Mmf6ZdIhuZAma+qP+dAS8kxumDkPXvn+dnrx3795mAYxTQplYOLNs2TLH4WqnslLzOM1iatK+AteiWdRBp1nU8bPSX/TDdaV7n2dl/KihAQuf9uFCy2M0iyKLl66Rh6J6yYmTpwTDzlMnDpGopnXli4XL5KlnxsrPW3Z45nN/Tuqk/pwI8bgdAcwthLkL5YrlTp06yRtvvJFuhp99fGgWw/y3QrOoq2CaRR0/K/1VvbWwvDDmLRnYu1PAwpdv3E+zGGAYukrttiaq2CmmiSyMXSmHDh+TW8qUkNjvVkm2bJHStWNzeXlYT0YWA8yZpf50v2nm9iYBmsUwr3eaRV0F0yzq+FF/On5W+stRoLr06NJCRgzpJqvXbpbK97SRiIgs0immsQzs014KFgj8fV0OQ+vqw2uRbR0t5g4nAjSL4VSbFs/CzlpXwTSLOn7B6m/qW2Nl1LABsmnP34wsBogsZshdyUQO8f3nXXsOSPFyD8msd4ZLi2Z1HSuLZtEeEfXnKCGe4FECNIthXvHBdtZOOLz2Zk2z6KQI++PB6u/tCaPlxaH9hFvnXORqpT8rszh35ihp1LCmY2XRLNojov4cJcQTPEqAZjHMKz7YztoJB82iEyH74+ys2VkHo6BAZvGGIgXNVjnxZ87Iug3bpHTJGyRvntwJRa+MnWp5GeqP+gtGfzyXBHwEaBbDXAs0i7oKZmRRxy9Y/TGyczlvK/3h035O6eShpTSLDpvCWwGi/pyUxeNeJUCzGOY1H2xn7YSDkUUnQows+hOw0h/mJAZKG9b+KCuXLeYw9P8B8WVF93uj/nT8mJsEGFn0iAZoFnUVzc5ax89Kf3af+fNdjXMWL5Kg/qg/HQHmJoHQEGBkMTQc02wpNIu6qmFnreNnpb9N69c4Flqu/O2W5zCy7YjO9gTOWRSh/nQaYm5vEqBZDPN6p1nUVTDNoo4f9afjR/3p+FF/On7MTQIchvaIBthY6iqanbWOH/Wn40f96fhRfzp+zE0CNIse0QAbS11Fs7PW8aP+dPyoPx0/6k/Hj7lJgGbRIxpgY6mraHbWOn7Un44f9afjR/3p+DE3CdAsekQDbCx1Fc3OWseP+tPxo/50/Kg/HT/mJgGaRY9ogI2lrqLZWev4UX86ftSfjh/1p+PH3CRAs+gRDbCx1FU0O2sdP+pPx4/60/Gj/nT8mJsEaBY9ooHRs9fJroN/hOxpixXKI0+2qBCy8tJ6QbtXDpRTv8eF7Daz5y0pRau8ELLy0npB1J+uhqg/HT/qT8ePuUnAR+B/Ke8Z/fXWXasAAAAASUVORK5CYII=)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJGw4gsSFdyL"
      },
      "source": [
        "class ReplayBuffer:\n",
        "  def __init__(self, capacity, rng):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    capacity : int\n",
        "      Maximum number of transitions\n",
        "    rng :\n",
        "      instance of numpy's default_rng\n",
        "    \"\"\"\n",
        "    self.capacity = capacity\n",
        "    self.rng = rng  # random number generator\n",
        "    self.memory = []\n",
        "    self.position = 0\n",
        "\n",
        "  def push(self, sample):\n",
        "    \"\"\"Saves a transition.\"\"\"\n",
        "    if len(self.memory) < self.capacity:\n",
        "      self.memory.append(None)\n",
        "    self.memory[self.position] = sample\n",
        "    self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    indices = self.rng.choice(len(self.memory), size=batch_size)\n",
        "    samples = [self.memory[idx] for idx in indices]\n",
        "    return map(np.asarray, zip(*samples))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.memory)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NPupSRGFQaV"
      },
      "source": [
        "class QNet(nn.Module):\n",
        "  def __init__(self, obs_size, n_actions):\n",
        "    super(QNet, self).__init__()\n",
        "    self.fc1 = nn.Linear(obs_size, n_actions)\n",
        "    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "    #  TO DO : Implement network & Forward method\n",
        "\n",
        "    self.fc1 = nn.Linear(obs_size, 64)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.fc2 = nn.Linear(64, 256)\n",
        "    self.fc3 = nn.Linear(256, n_actions)\n",
        "\n",
        "    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "\n",
        "  def forward(self, state):\n",
        "    #We do some implementation here due to previous implementations above.\n",
        "    Q = self.relu(self.fc1(state))\n",
        "    Q = self.relu(self.fc2(Q))\n",
        "    Q = self.fc3(Q)\n",
        "    return Q\n",
        "\n",
        "\n",
        "class DuelingQNet(nn.Module):\n",
        "  def __init__(self, obs_size, n_actions):\n",
        "    super(DuelingQNet, self).__init__()\n",
        "    self.fc1 = nn.Linear(obs_size, n_actions)\n",
        "    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "    #  TO DO : Implement network & Forward method\n",
        "    self.relu = nn.ReLU()\n",
        "    self.fc1 = nn.Linear(obs_size, 64)\n",
        "    self.fc2_value = nn.Linear(64, 64)\n",
        "    self.fc2_advantage = nn.Linear(64,64)\n",
        "    self.fc3_value = nn.Linear(64, 1)\n",
        "    self.fc3_advantage = nn.Linear(64, n_actions)\n",
        "    self.num_actions = n_actions\n",
        "\n",
        "    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "\n",
        "  def forward(self, state):\n",
        "    x = self.relu(self.fc1(state))\n",
        "    V = self.relu(self.fc2_value(x))\n",
        "    A = self.relu(self.fc2_advantage(x))\n",
        "    V = self.fc3_value(V)\n",
        "    A = self.fc3_advantage(A)\n",
        "    return A - A.mean(1, keepdim=True) + V"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bceC1urVPaZM"
      },
      "source": [
        "# Parameters\n",
        "DQN_TRAINING_TIMESTEPS = 10000  # number of timesteps for training. You might change this!\n",
        "\n",
        "DQN_PARAMS = dict(\n",
        "    dueling_dqn=False,         # set to true to use dueling DQN\n",
        "    double_dqn=False,          # set to true to use double DQN\n",
        "    gamma=0.99,\n",
        "    batch_size=256,            # batch size (in number of transitions)\n",
        "    eval_every=250,            # evaluate every ... steps\n",
        "    buffer_capacity=30000,     # capacity of the replay buffer\n",
        "    update_target_every=500,   # update target net every ... steps\n",
        "    epsilon_start=1.0,         # initial value of epsilon\n",
        "    epsilon_min=0.05,          # minimum value of epsilon\n",
        "    decrease_epsilon=5000,     # parameter to decrease epsilon\n",
        "    learning_rate=0.001,       # learning rate\n",
        ")\n",
        "DUELING_DQN_PARAMS = deepcopy(DQN_PARAMS)           # dueling DQN\n",
        "DOUBLE_DQN_PARAMS = deepcopy(DQN_PARAMS)            # double DQN\n",
        "DOUBLE_DUELING_DQN_PARAMS = deepcopy(DQN_PARAMS)    # double & dueling DQN\n",
        "\n",
        "DUELING_DQN_PARAMS.update(dict(dueling_dqn=True))\n",
        "DOUBLE_DQN_PARAMS.update(dict(double_dqn=True))\n",
        "DOUBLE_DUELING_DQN_PARAMS.update(dueling_dqn=True, double_dqn=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTiVIsyTF62S"
      },
      "source": [
        "class DQNAgent(Agent):\n",
        "  name = 'DQN'\n",
        "  def __init__(\n",
        "      self,\n",
        "      env,\n",
        "      dueling_dqn: bool,               # Set to true for dueling DQN\n",
        "      double_dqn: bool,                # Set to true for double DQN\n",
        "      gamma: float = 0.99,\n",
        "      batch_size: int = 256,\n",
        "      eval_every: int = 500,\n",
        "      buffer_capacity: int = 30000,\n",
        "      update_target_every: int = 500,\n",
        "      epsilon_start: float = 1.0,\n",
        "      epsilon_min: float = 0.05,\n",
        "      decrease_epsilon: int = 200,\n",
        "      learning_rate: float = 0.001,\n",
        "      **kwargs):\n",
        "    Agent.__init__(self, env, **kwargs)\n",
        "    env = self.env\n",
        "    self.dueling_dqn = dueling_dqn\n",
        "    self.double_dqn = double_dqn\n",
        "    self.gamma = gamma\n",
        "    self.batch_size = batch_size\n",
        "    self.eval_every = eval_every\n",
        "    self.update_target_every = update_target_every\n",
        "    self.epsilon_start = epsilon_start\n",
        "    self.epsilon_min = epsilon_min\n",
        "    self.decrease_epsilon = decrease_epsilon\n",
        "    self.total_timesteps = 0\n",
        "    self.total_episodes = 0\n",
        "    self.total_updates = 0\n",
        "\n",
        "    # initialize epsilon\n",
        "    self.epsilon = epsilon_start\n",
        "\n",
        "    # initialize replay buffer\n",
        "    self.replay_buffer = ReplayBuffer(buffer_capacity, self.rng)\n",
        "\n",
        "    # select network class\n",
        "    if self.dueling_dqn:\n",
        "      net_class = DuelingQNet\n",
        "    else:\n",
        "      net_class = QNet\n",
        "\n",
        "    # update name according to params\n",
        "    if self.dueling_dqn:\n",
        "      self.name = 'Dueling' + self.name\n",
        "    if self.double_dqn:\n",
        "      self.name = 'Double' + self.name\n",
        "\n",
        "    # create network and target network\n",
        "    obs_size = env.observation_space.shape[0]\n",
        "    n_actions = env.action_space.n\n",
        "    self.q_net = net_class(obs_size, n_actions)\n",
        "    self.target_net = net_class(obs_size, n_actions)\n",
        "    self.target_net.load_state_dict(self.q_net.state_dict())\n",
        "    self.target_net.eval()\n",
        "\n",
        "    # objective and optimizer\n",
        "    self.optimizer = optim.Adam(\n",
        "        params=self.q_net.parameters(), lr=learning_rate)\n",
        "    self.loss_fn = nn.MSELoss()\n",
        "\n",
        "  def select_action(self, state, evaluation=False):\n",
        "    \"\"\"\n",
        "    If evaluation=False, get action according to exploration policy.\n",
        "    Otherwise, get action according to the evaluation policy.\n",
        "    \"\"\"\n",
        "    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "    # TODO: implement action selection strategy\n",
        "    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "    # epsilon greedy strategy\n",
        "    if self.rng.uniform() < self.epsilon and (not evaluation):\n",
        "      # With probability e, we select arbitrary an action\n",
        "      action = np.random.choice(self.env.action_space.n)\n",
        "    else:\n",
        "      with torch.no_grad():\n",
        "        # With proba 1-e, we take the greedy action\n",
        "        BatchState = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "        q = self.q_net(BatchState)\n",
        "        action = torch.argmax(q).item()\n",
        "    return action # replace by the action you computed\n",
        "\n",
        "  def fit(self, budget):\n",
        "    \"\"\"\n",
        "    budget : number of training timesteps\n",
        "    \"\"\"\n",
        "    state = self.env.reset()\n",
        "    done = False\n",
        "    episode_reward = 0.0\n",
        "    for tt in range(budget):\n",
        "      self.total_timesteps += 1\n",
        "      action = self.select_action(state, evaluation=False)\n",
        "      next_state, reward, done, _ = self.env.step(action)\n",
        "      episode_reward += reward\n",
        "      self.replay_buffer.push((state, next_state, action, reward, done))\n",
        "\n",
        "      if len(self.replay_buffer) > self.batch_size:\n",
        "        #\n",
        "        # Update model\n",
        "        #\n",
        "        self.total_updates += 1\n",
        "\n",
        "        # get batch\n",
        "        (BatchState, batch_next_state,\n",
        "         batch_action, batch_reward,\n",
        "         batch_done) = self.replay_buffer.sample(self.batch_size)\n",
        "        # convert to torch tensors\n",
        "        BatchState = torch.FloatTensor(BatchState).to(device)\n",
        "        batch_next_state = torch.FloatTensor(batch_next_state).to(device)\n",
        "        batch_action = torch.LongTensor(batch_action).unsqueeze(1).to(device)\n",
        "        batch_reward = torch.FloatTensor(batch_reward).unsqueeze(1).to(device)\n",
        "        batch_done = torch.FloatTensor(batch_done).unsqueeze(1).to(device)\n",
        "\n",
        "        # decrease epsilon\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon -= (self.epsilon_start\n",
        "                             - self.epsilon_min) / self.decrease_epsilon\n",
        "\n",
        "        #\n",
        "        #  TO DO: compute loss and update networks\n",
        "        #\n",
        "        if not self.double_dqn:\n",
        "          # !!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "          # TO DO: compute DQN targets\n",
        "          values = self.q_net(BatchState).gather(1, batch_action.long())\n",
        "          with torch.no_grad():\n",
        "            # Take the max in a' of Q(s', a') as estimate for next state value\n",
        "            value_next_state, _ = torch.max(self.target_net(batch_next_state), dim=1, keepdim=True)\n",
        "            targets = batch_reward + (self.gamma * value_next_state) * (1 - batch_done)\n",
        "\n",
        "          # !!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "\n",
        "          with torch.no_grad():\n",
        "            targets = torch.zeros_like(values)\n",
        "\n",
        "        else:\n",
        "          # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "          # TO DO: compute Double DQN targets\n",
        "          # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "          values = self.q_net(BatchState).gather(1, batch_action.long())\n",
        "          with torch.no_grad():\n",
        "            # Take the 'unbiased' estimate of the greedy action on the next stape using Qnet\n",
        "            greedy_actions = torch.argmax(self.q_net(batch_next_state), dim=1, keepdim=True)\n",
        "            # Evaluate the next state value using the target net, for avoiding overoptimistic estimates\n",
        "            value_next_state = self.target_net(batch_next_state).gather(1, greedy_actions.long())\n",
        "            targets = batch_reward + (self.gamma * value_next_state) * (1 - batch_done)\n",
        "\n",
        "        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "        # TO DO: compute loss and take gradient step\n",
        "        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "        # loss = torch.tensor(0.0)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss = self.loss_fn(values, targets)\n",
        "        loss.backward()\n",
        "        # # Clipping gradient to 1\n",
        "        nn.utils.clip_grad_norm_(self.q_net.parameters(), 1)\n",
        "        self.optimizer.step()\n",
        "        self.writer.add_scalar('loss', loss.item(), self.total_timesteps)\n",
        "\n",
        "      # evaluate agent\n",
        "      if self.total_timesteps % self.eval_every == 0:\n",
        "          mean_rewards = self.eval(n_sim=5)\n",
        "          self.writer.add_scalar(\n",
        "              'eval_rewards', mean_rewards, self.total_timesteps)\n",
        "\n",
        "      # update target network\n",
        "      if self.total_updates % self.update_target_every == 0:\n",
        "          self.target_net.load_state_dict(self.q_net.state_dict())\n",
        "          self.target_net.eval()\n",
        "\n",
        "      # check end of episode\n",
        "      state = next_state\n",
        "      if done:\n",
        "        state = self.env.reset()\n",
        "        self.total_episodes += 1\n",
        "        self.writer.add_scalar(\n",
        "                'episode_rewards', episode_reward, self.total_timesteps)\n",
        "        self.writer.add_scalar(\n",
        "                'episode', self.total_episodes, self.total_timesteps)\n",
        "        episode_reward = 0.0\n",
        "\n",
        "  def eval(self, n_sim=1, **kwargs):\n",
        "    rewards = np.zeros(n_sim)\n",
        "    eval_env = self.eval_env     # evaluation environment\n",
        "    # Loop over number of simulations\n",
        "    for sim in range(n_sim):\n",
        "      state = eval_env.reset()\n",
        "      done = False\n",
        "      while not done:\n",
        "        action = self.select_action(state, evaluation=True)\n",
        "        next_state, reward, done, _ = eval_env.step(action)\n",
        "        # update sum of rewards\n",
        "        rewards[sim] += reward\n",
        "        state = next_state\n",
        "    return rewards.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1ohNvaIGLJo"
      },
      "source": [
        "# # Training one instance of DQN\n",
        "# dqn_agent = DQNAgent(\n",
        "#     env=(get_env, dict()),  # we can send (constructor, kwargs) as an env\n",
        "#     **DQN_PARAMS\n",
        "# )\n",
        "# dqn_agent.fit(DQN_TRAINING_TIMESTEPS)\n",
        "\n",
        "#\n",
        "# Training several instances using AgentManager\n",
        "#\n",
        "manager_kwargs = dict(\n",
        "    agent_class=DQNAgent,\n",
        "    train_env=(get_env, dict()),\n",
        "    eval_env=(get_env, dict()),\n",
        "    fit_budget=DQN_TRAINING_TIMESTEPS,\n",
        "    n_fit=2,                   # NOTE: You may increase this parameter (number of agents to train)\n",
        "    parallelization='thread',\n",
        "    seed=456,\n",
        "    default_writer_kwargs=dict(maxlen=None,log_interval=10),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eBK0swvwakz"
      },
      "source": [
        "# DQN\n",
        "dqn_manager = AgentManager(\n",
        "    init_kwargs=DQN_PARAMS,\n",
        "    agent_name='DQN',\n",
        "    **manager_kwargs\n",
        ")\n",
        "dqn_manager.fit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M27dcKmzwDIL"
      },
      "source": [
        "# Double DQN\n",
        "double_dqn_manager = AgentManager(\n",
        "    init_kwargs=DOUBLE_DQN_PARAMS,\n",
        "    agent_name='DoubleDQN',\n",
        "    **manager_kwargs\n",
        ")\n",
        "double_dqn_manager.fit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDWEASGwwHUW"
      },
      "source": [
        "# Dueling DQN\n",
        "dueling_dqn_manager = AgentManager(\n",
        "    init_kwargs=DUELING_DQN_PARAMS,\n",
        "    agent_name='DuelingDQN',\n",
        "    **manager_kwargs\n",
        ")\n",
        "dueling_dqn_manager.fit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdQzmVcUwJXg"
      },
      "source": [
        "# Double+Dueling DQN\n",
        "double_dueling_dqn_manager = AgentManager(\n",
        "    init_kwargs=DOUBLE_DUELING_DQN_PARAMS,\n",
        "    agent_name='DoubleDuelingDQN',\n",
        "    **manager_kwargs\n",
        ")\n",
        "double_dueling_dqn_manager.fit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aKcY8CYwfDl"
      },
      "source": [
        "all_dqn_managers = []\n",
        "all_dqn_managers.append(dqn_manager)\n",
        "all_dqn_managers.append(double_dqn_manager)\n",
        "all_dqn_managers.append(dueling_dqn_manager)\n",
        "all_dqn_managers.append(double_dueling_dqn_manager)\n",
        "\n",
        "# We can plot the data that was\n",
        "# stored by the agent with self.writer.add_scalar(tag, value, global_step):\n",
        "_ = plot_writer_data(all_dqn_managers, tag='loss', title='Q Loss')\n",
        "_ = plot_writer_data(all_dqn_managers, tag='eval_rewards', title='Rewards (Evaluation)')\n",
        "_ = plot_writer_data(all_dqn_managers, tag='eval_rewards', title='Cumulative Rewards (Evaluation)', preprocess_func=np.cumsum)\n",
        "\n",
        "# rewards in each episode\n",
        "# Warning: not all runs will have the same number of episodes. Can you see why?\n",
        "_ = plot_writer_data(all_dqn_managers, tag='episode_rewards', xtag='episode', title='Episode Rewards')\n",
        "_ = plot_writer_data(all_dqn_managers, tag='episode_rewards', xtag='episode', title='Cumulative Rewards', preprocess_func=np.cumsum)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNhMb-vfpTOS"
      },
      "source": [
        "# Part 2: REINFORCE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqRB3x2uod1a"
      },
      "source": [
        "\n",
        "## Question 2.1 (written)\n",
        "\n",
        "In class we have seen the derivation of the policy gradient theorem in the \"Monte-Carlo style\". Recall that the policy gradient is given by\n",
        "$$\n",
        "    \\nabla_\\theta J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\mathbb{P}(\\cdot|\\pi_\\theta)}\\left[ \\nabla_\\theta \\log \\mathbb{P}(\\tau|\\pi_\\theta)\n",
        "    R(\\tau)\n",
        "    \\right]\n",
        "$$\n",
        "where $R(\\tau) = \\sum_{t=1}^{|\\tau|} r_t$. By construction the policy gradient is on-policy, we need to estimate the gradient using the samples collected through the current policy $\\pi_\\theta$.\n",
        "\n",
        "1. *Derive an off-policy variant* by assuming to collect samples from a behavioral policy $\\mu(s,a)$. The target policy, i.e., the policy for which we want to compute the gradient is $\\pi_{\\theta}$. Write explicitly the gradient log probability in your derivation.\n",
        "\n",
        "2. What are the properties that $\\mu$ needs to satisfy to obtain a meaningful gradient estimate?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmmxS4XTpKsw"
      },
      "source": [
        "####ANSWERS\n",
        "\n",
        "**1)** We will derive the off-policy variant from the policy gradient given by\n",
        " $$\n",
        "    \\nabla_\\theta J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\mathbb{P}(\\cdot|\\pi_\\theta)}\\left[ \\nabla_\\theta \\log \\mathbb{P}(\\tau|\\pi_\\theta)\n",
        "    R(\\tau)\n",
        "    \\right]\n",
        "$$\n",
        "\n",
        "Before we derivate the off policy, we firstly calculate importance ratio which is the ratio $\\frac{Π_θ}{μ}$\n",
        "\n",
        "$$ \\dfrac{ \\pi_θ(τ)}{\\mu(τ)}=  \\dfrac{p(s_1) \\prod_{t=1}^{|\\tau|} \\pi_θ(a_t | s_t)p(s_{t+1}|s_t,a_t)}{p(s_1) \\prod_{t=1}^{|\\tau|} \\mu(a_t | s_t)p(s_{t+1}|s_t,a_t)}\n",
        "$$\n",
        "\n",
        "$$\\dfrac{ \\pi_θ(τ)}{\\mu(τ)}=\\dfrac{∏_{t=1}^{|\\tau|} \\pi_θ(a_t|s_t)}{∏_{t=1}^{|\\tau|} \\mu(a_t|s_t)}\n",
        "$$\n",
        "\n",
        "We have now:\n",
        "\n",
        "$$\\nabla_\\theta \\mathbb{E}_{\\tau \\sim \\mathbb{P}(\\cdot|\\mu)})\\left[ \\frac{ \\pi_θ(τ)}{\\mu(τ)}R(τ) \\right] = \\nabla_\\theta \\mathbb{E}_{\\tau \\sim \\mathbb{P}(\\cdot|\\mu)} \\left[ \\frac{ \\pi_θ(τ)}{\\mu(τ)} ∇_θ \\log \\pi_θ(τ)R(τ) \\right]$$\n",
        "\n",
        "We now expand the equation to include the derived form of importance ratio that we calculated earlier.\n",
        "\n",
        "$$ \\mathbb{E}_{\\tau \\sim \\mathbb{P}(\\cdot|\\mu)}\\left[ \\left(  \\prod_{t=1}^{|\\tau|}  \\frac{\\pi_\\theta(a_t|s_t)}{\\mu(a_t|s_t)} \\right) \\sum_{t=1}^{|\\tau|}  \\nabla_θ \\log \\pi_θ(a_t|s_t) \\left( \\sum_{t=1}^{|\\tau|}r(s_t|a_t) \\right) \\right]  $$\n",
        "\n",
        "\n",
        "**2)** $\\mu$ needs to comprise $\\pi$. Meaning that, exploratory policy sometimes needs to select every possible action and visit every state. Because we do not know in advance what action the optimal policy will be chosen.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUJhgy5NpK-o"
      },
      "source": [
        "## Question 2.2 (implementation)\n",
        "Starting from the provided code, implement on-policy policy gradient\n",
        "$$\n",
        "    \\nabla_\\theta J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\mathbb{P}(\\cdot|\\pi_\\theta)}\\left[ \\nabla_\\theta \\log \\mathbb{P}(\\tau|\\pi_\\theta)\n",
        "    R(\\tau)\n",
        "    \\right]\n",
        "$$\n",
        "\n",
        "Use the most efficient version of the gradient estimator without baseline (i.e., GPOMDP)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Nd0KBPRzeVU"
      },
      "source": [
        "class PolicyNet(nn.Module):\n",
        "  def __init__(self, obs_size, n_actions):\n",
        "    super(PolicyNet, self).__init__()\n",
        "    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "    #  TO DO : Implement network & Forward method\n",
        "    #  You can use the same architecture as DQN\n",
        "    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "    self.fc1 = nn.Linear(obs_size, 64)\n",
        "    self.fc2 = nn.Linear(64, 64)\n",
        "    self.fc3 = nn.Linear(64, n_actions)\n",
        "    self.softmax = nn.Softmax(dim=-1)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    #my implementation is done !!!!\n",
        "\n",
        "  def forward(self, state):\n",
        "    action_logits = self.fc1(state)\n",
        "    action_logits = self.relu(action_logits)\n",
        "    action_logits = self.fc2(action_logits)\n",
        "    action_logits = self.relu(action_logits)\n",
        "    action_logits = self.fc3(action_logits)\n",
        "\n",
        "    # Made some implementations due to implementations above.\n",
        "    #my implementation is done !!!!\n",
        "\n",
        "    # compute softmax of the output of the net, to get probabilities over actions\n",
        "    action_probs = self.softmax(action_logits)\n",
        "    dist = Categorical(action_probs)\n",
        "    return action_logits, dist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExSW-AqZp59y"
      },
      "source": [
        "# Parameters\n",
        "REINFORCE_TRAINING_TIMESTEPS = 5*DQN_TRAINING_TIMESTEPS  # number of timesteps for training. You might change this!\n",
        "\n",
        "REINFORCE_PARAMS = dict(\n",
        "    gamma=0.99,\n",
        "    batch_size=200,            # batch size\n",
        "    eval_every=250,            # evaluate every ... steps\n",
        "    learning_rate=0.001,       # learning rate\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyabbOKar7fx"
      },
      "source": [
        "class REINFORCEAgent(Agent):\n",
        "  name = 'REINFORCE'\n",
        "  def __init__(\n",
        "      self,\n",
        "      env,\n",
        "      gamma: float = 0.99,\n",
        "      batch_size: int = 200,\n",
        "      eval_every: int = 250,\n",
        "      learning_rate: float = 0.1,\n",
        "      **kwargs):\n",
        "    Agent.__init__(self, env, **kwargs)\n",
        "    env = self.env\n",
        "    self.gamma = gamma\n",
        "    self.batch_size = batch_size\n",
        "    self.eval_every = eval_every\n",
        "    self.total_timesteps = 0\n",
        "    self.total_episodes = 0\n",
        "    self.total_updates = 0\n",
        "\n",
        "    # create network\n",
        "    obs_size = env.observation_space.shape[0]\n",
        "    n_actions = env.action_space.n\n",
        "    self.policy_net = PolicyNet(obs_size, n_actions)\n",
        "\n",
        "    # optimizer\n",
        "    self.optimizer = optim.Adam(\n",
        "        params=self.policy_net.parameters(), lr=learning_rate)\n",
        "\n",
        "  def select_action(self, state, evaluation=False):\n",
        "    \"\"\"\n",
        "    If evaluation=False, get action according to exploration policy.\n",
        "    Otherwise, get action according to the evaluation policy.\n",
        "    \"\"\"\n",
        "    tensor_state = torch.FloatTensor(state).to(device)\n",
        "    with torch.no_grad():\n",
        "      _, dist = self.policy_net(tensor_state)\n",
        "      action = dist.sample().item()\n",
        "    return action\n",
        "\n",
        "  def fit(self, budget):\n",
        "    \"\"\"\n",
        "    budget : number of training timesteps\n",
        "    \"\"\"\n",
        "    state = self.env.reset()\n",
        "    done = False\n",
        "    episode_reward = 0.0\n",
        "\n",
        "    TrajectoryStates = []\n",
        "    TrajectoryActions = []\n",
        "    TrajectoryRewards = []\n",
        "    TrajectoryDones = []\n",
        "\n",
        "    for tt in range(budget):\n",
        "      self.total_timesteps += 1\n",
        "      action = self.select_action(state, evaluation=False)\n",
        "      next_state, reward, done, _ = self.env.step(action)\n",
        "      episode_reward += reward\n",
        "\n",
        "      TrajectoryStates.append(state)\n",
        "      TrajectoryActions.append(action)\n",
        "      TrajectoryRewards.append(reward)\n",
        "      TrajectoryDones.append(done)\n",
        "\n",
        "      if ((tt + 1) % self.batch_size) == 0:\n",
        "        #\n",
        "        # Update model\n",
        "        #\n",
        "        self.total_updates += 1\n",
        "\n",
        "        # convert to torch tensors\n",
        "        BatchState = torch.FloatTensor(\n",
        "            np.array(TrajectoryStates, dtype=float)\n",
        "            ).to(device)\n",
        "        batch_action = torch.LongTensor(\n",
        "             np.array(TrajectoryActions, dtype=int)\n",
        "             ).unsqueeze(1).to(device)\n",
        "        batch_reward = torch.FloatTensor(\n",
        "             np.array(TrajectoryRewards, dtype=float)\n",
        "            ).unsqueeze(1).to(device)\n",
        "        batch_done = torch.FloatTensor(\n",
        "            np.array(TrajectoryDones, dtype=bool)\n",
        "            ).unsqueeze(1).to(device)\n",
        "\n",
        "        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "        #  TO DO: compute loss for REINFORCE\n",
        "\n",
        "        assert len(TrajectoryActions) == self.batch_size\n",
        "\n",
        "        action_logits, _ = self.policy_net(BatchState)\n",
        "        logprobs = nn.functional.log_softmax(action_logits, dim=-1).gather(1, batch_action.long())\n",
        "        loss = torch.tensor(0.0).to(device)\n",
        "\n",
        "        LastIndex = -2 #we start from -2 as last index\n",
        "        number_of_episodes = batch_done.sum()\n",
        "        DiscountedRewards = [] #Discounted reward list\n",
        "        DiscReward = 0 #Discounted reward\n",
        "        for batch_item in range(self.batch_size - 1, -1, -1):\n",
        "          # When episode is finished, we compute the sum of total reward and restart the discount\n",
        "          if batch_done[batch_item]:\n",
        "            # Semi-gradient version\n",
        "            DiscountedRewards = torch.flip(torch.tensor(DiscountedRewards), (0,)).to(device)\n",
        "            trajectory_start = batch_item + 1\n",
        "            trajectory_end = LastIndex + 1\n",
        "            loss += (logprobs[trajectory_start:trajectory_end] * DiscountedRewards).sum()\n",
        "            # Restarting discount\n",
        "            DiscReward = 0\n",
        "            DiscountedRewards = []\n",
        "            # Updating trajectory index\n",
        "            LastIndex = batch_item\n",
        "\n",
        "\n",
        "          # Running count of discounted reward\n",
        "          DiscReward = self.gamma * DiscReward + batch_reward[batch_item]\n",
        "          DiscountedRewards.append(DiscReward)\n",
        "\n",
        "        # Discounted rewards for the first trajectory\n",
        "        DiscountedRewards = torch.flip(torch.tensor(DiscountedRewards), (0,)).to(device)\n",
        "        trajectory_end = LastIndex +1\n",
        "        loss += (logprobs[:trajectory_end] * DiscountedRewards).sum()\n",
        "        loss /= (number_of_episodes+1)\n",
        "        loss = - loss\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "\n",
        "        # log info about loss\n",
        "        self.writer.add_scalar('loss', loss.item(), self.total_timesteps)\n",
        "\n",
        "        # clear data\n",
        "        TrajectoryStates = []\n",
        "        TrajectoryActions = []\n",
        "        TrajectoryRewards = []\n",
        "        TrajectoryDones = []\n",
        "\n",
        "      # evaluate agent\n",
        "      if self.total_timesteps % self.eval_every == 0:\n",
        "          mean_rewards = self.eval(n_sim=5)\n",
        "          self.writer.add_scalar(\n",
        "              'eval_rewards', mean_rewards, self.total_timesteps)\n",
        "\n",
        "      # check end of episode\n",
        "      state = next_state\n",
        "      if done:\n",
        "        state = self.env.reset()\n",
        "        self.total_episodes += 1\n",
        "        self.writer.add_scalar(\n",
        "                'episode_rewards', episode_reward, self.total_timesteps)\n",
        "        self.writer.add_scalar(\n",
        "                'episode', self.total_episodes, self.total_timesteps)\n",
        "        episode_reward = 0.0\n",
        "\n",
        "  def eval(self, n_sim=1, **kwargs):\n",
        "    rewards = np.zeros(n_sim)\n",
        "    eval_env = self.eval_env     # evaluation environment\n",
        "    # Loop over number of simulations\n",
        "    for sim in range(n_sim):\n",
        "      state = eval_env.reset()\n",
        "      done = False\n",
        "      while not done:\n",
        "        action = self.select_action(state, evaluation=True)\n",
        "        next_state, reward, done, _ = eval_env.step(action)\n",
        "        # update sum of rewards\n",
        "        rewards[sim] += reward\n",
        "        state = next_state\n",
        "    return rewards.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4n4PJgTqujup"
      },
      "source": [
        "# # Training one instance of REINFORCE\n",
        "# reinforce_agent = REINFORCEAgent(\n",
        "#     env=(get_env, dict()),  # we can send (constructor, kwargs) as an env\n",
        "#     **REINFORCE_PARAMS\n",
        "# )\n",
        "# reinforce_agent.fit(REINFORCE_TRAINING_TIMESTEPS)\n",
        "\n",
        "#\n",
        "# Training several instances using AgentManager\n",
        "#\n",
        "manager_kwargs = dict(\n",
        "    agent_class=REINFORCEAgent,\n",
        "    train_env=(get_env, dict()),\n",
        "    eval_env=(get_env, dict()),\n",
        "    fit_budget=REINFORCE_TRAINING_TIMESTEPS,\n",
        "    n_fit=2,                # NOTE: You may increase this parameter (number of agents to train)\n",
        "    parallelization='thread',\n",
        "    seed=456,\n",
        "    default_writer_kwargs=dict(maxlen=None,log_interval=10),\n",
        ")\n",
        "\n",
        "# REINFORCE\n",
        "reinforce_manager = AgentManager(\n",
        "    init_kwargs=REINFORCE_PARAMS,\n",
        "    agent_name='REINFORCE',\n",
        "    **manager_kwargs\n",
        ")\n",
        "\n",
        "# Train and append to previous managers\n",
        "reinforce_manager.fit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hsr549uq11uH"
      },
      "source": [
        "# Plots for REINFORCE\n",
        "_ = plot_writer_data(reinforce_manager, tag='loss', title='Policy Loss')\n",
        "_ = plot_writer_data(reinforce_manager, tag='episode_rewards', xtag='episode', title='Episode Rewards')\n",
        "_ = plot_writer_data(reinforce_manager, tag='eval_rewards', title='Rewards (Evaluation)')\n",
        "_ = plot_writer_data(reinforce_manager, tag='eval_rewards', title='Cumulative Rewards (Evaluation)', preprocess_func=np.cumsum)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpvRJljOz2Pb"
      },
      "source": [
        "#Part 3: Model Selection Using UCB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IaNj91pqZ5D"
      },
      "source": [
        "## Question 3.1: Model Selection for RL\n",
        "\n",
        "\n",
        "In RL, it is hard to define in advance what is the best method to use and/or which architecture to choose for representing the policy or the value function.\n",
        "\n",
        "Normally, several algorithms are run in parallel and the final performance is used to select the best algorithm. Can we do this online?\n",
        "\n",
        "In this exercise, we present a simple idea for doing online model-selection by leveraging a multi-armed bandit (MAB) algorithm.\n",
        "\n",
        "You will start from a list containing one instance of each agent that you trained above (different versions of DQN and REINFORCE), and you'll adaptively allocate computational resources to train those algorithms for more timesteps.\n",
        "\n",
        "**Implement below the UCB algorithm such that, at reach round $t$:**    \n",
        "\n",
        "* UCB selects the action $a_t$, which corresponds to one of the algorithms in the list;\n",
        "* You train the algorithm for a few timesteps;\n",
        "* UCB receives the reward $r_t$ which is equal to the Monte-Carlo policy evaluation of the chosen algorithm.\n",
        "\n",
        "The goal of this strategy is to allocate more computational resources to the algorithms that are yielding more rewards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9JZxc7Rz6ov"
      },
      "source": [
        "# Get one (pre-)trained agent from each manager\n",
        "all_managers = all_dqn_managers + [reinforce_manager]\n",
        "arms = [manager.get_agent_instances()[0] for manager in all_managers]\n",
        "print(f\"Arms/algorithms: {[alg.name for alg in arms]}\")\n",
        "\n",
        "# Select number of UCB rounds\n",
        "UCB_N_ROUNDS = 200\n",
        "\n",
        "# Number of training steps every time an agent is selected\n",
        "N_STEPS_TO_TRAIN = 250\n",
        "\n",
        "# Run UCB\n",
        "all_episode_rewards = []\n",
        "arm_counts = np.zeros(len(arms))\n",
        "arm_cumulative_rewards = np.zeros(len(arms))\n",
        "for tt in range(UCB_N_ROUNDS):\n",
        "  if tt < len(arms):\n",
        "    arm_index = tt\n",
        "  else:\n",
        "    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "    # TODO: Implement UCB arm selection\n",
        "    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "    arm_index = 0\n",
        "\n",
        "  # train the selected algorithm for more timesteps\n",
        "  arms[arm_index].fit(N_STEPS_TO_TRAIN)\n",
        "\n",
        "  # evaluate arm\n",
        "  episode_reward = arms[arm_index].eval()\n",
        "\n",
        "  # update stats\n",
        "  arm_counts[arm_index] += 1\n",
        "  arm_cumulative_rewards[arm_index] += episode_reward\n",
        "\n",
        "  # store reward\n",
        "  all_episode_rewards.append(episode_reward)\n",
        "\n",
        "  if (tt+1) % 10 == 0:\n",
        "    print(\"-----------------------------------------------------------\")\n",
        "    print(f\"UCB round {tt+1}, \"\n",
        "          f\"chosen algorithm: {arms[arm_index].name}, \"\n",
        "          f\"eval reward = {episode_reward}.\")\n",
        "    print(\"-----------------------------------------------------------\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6KjxoLbqQZb"
      },
      "source": [
        "# Print stats\n",
        "print(f\"Arms: {[alg.name for alg in arms]}\")\n",
        "print(f\"Counts: {arm_counts}\")\n",
        "print(f\"Mean arm rewards: {arm_cumulative_rewards/arm_counts}\")\n",
        "\n",
        "# Plot rewards and regret\n",
        "all_episode_rewards = np.array(all_episode_rewards)\n",
        "plt.figure()\n",
        "plt.plot(np.cumsum(all_episode_rewards))\n",
        "plt.title(\"Cumulative Rewards\")\n",
        "plt.figure()\n",
        "plt.plot(np.cumsum(200.0 - all_episode_rewards))  # maximum reward in CartPole-v0 is 200.0\n",
        "plt.title(\"Regret\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_WqSuZ2qHIL"
      },
      "source": [
        "## Question 3.2: Is UCB well-suited for online model-selection?\n",
        "\n",
        "At every round of UCB, one algorithm is selected and trained for a few timesteps. Does this break any assumption made by the UCB algorithm? If so, how would you handle this issue?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytEVfM_BrAA7"
      },
      "source": [
        "####ANSWERS\n",
        "\n",
        "\n",
        "An assumption broken in UCB is that the basic distribution of rewards by each arm is fixed over time. In this case, we train each model for several time steps while changing the distribution of rewards for the selected arm.\n",
        "\n",
        "As a workaround, it may be suggested to create buffers to store the last interactions with each arm. The average reward and UCB can then be calculated based on this buffer. In this way, the agent can forget about old interactions that no longer represent new reward distributions and adapt the decision rule to the latest experience."
      ]
    }
  ]
}